{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project milestone 4\n",
    "Tadaa - Jonathan Haenni, Lea Schmidt, Danny Kohler\n",
    "> This work presents the creative extension of the publication from J. Penney : \"Chilling effects : Online surveillance and Wikipedia use\".\n",
    "\n",
    "---\n",
    "\n",
    "## Abstract\n",
    "The goal of this work is to observe if the public action of Greta Thunberg and her continual incitation for people to educate themselves on the subject of climate change has actually made a difference. This effect would be diametrically different from a chilling effect in that it pushes people to get educated on a subject, effectively arguably increasing their levels of freedom. This effect shall henceforth be called an \"Empowering effect\".\n",
    "\n",
    "First and foremost, the selected interest period depends on the choice of events that will be the pivots of the interrupted time series analysis. (ITS) These events are as follows: the school strike of August 20, 2018 led by Greta Thunberg, the 2018 United Nations Climate Change Conference (COP24) on December 14, 2018 and the summit of the United Nations on September 23, 2019. The period of analysis is between January 2018 and February 2020. As seen in HW1, the Covid-19 pandemic influences the pageviews a lots and we will therefore not elongate this analysis onto 2020.\n",
    "\n",
    "The data considered here comes from Wikipedia. We will consider 150 [ADAPT VALUE] Wikipedia articles divided into 3 groups. The first group is the treatment dataset, containing the Wikipedia articles related to climate change issues. The second group is a quasi-control group, where the topics considered are related to nature, without being related to climate change directly. The third group is another control group composed of popular articles simply reflecting the trends on Wikipedia. It is the same as the one used in the publication. The data sets considered give the number of pageviews per day for each article, within the time period considered.\n",
    "\n",
    "As already hinted at, the analysis will be very similar to the one used in the publication - an ITS analysis with segmented linear regressions.\n",
    "\n",
    "If the first group features significant changes in pageviews and the control groups don't across the selected period, we will be able to conclude that the \"Empowering Effect\" exists and we will be able to compare it to the chilling effect considered in the publication.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytical Methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform the comparison between the periods, we will use a trivial mean comparison as a non-model empirical findings in a first time.\n",
    "\n",
    "Then we will use a customized interrupted time serie analysis as depicted by Lagarde explained further. The purpose is to use a unique linear regression through all the periods with a dataset taking into account the influence of the data only on its corresponding period. \n",
    "\n",
    ">> Lagarde ITS : $$y_t = \\gamma_0 + \\gamma_1  \\times preslope + \\gamma_2  \\times intervention + \\gamma_3  \\times postslope + \\varepsilon_t$$\n",
    ">\n",
    ">This is the equation used by Lagarde (2012) mentioned in the paper where $y_t$ is the outcome at time $t$, $\\gamma_0$ is the baseline level at time 0, $\\gamma_1$ is the estimation of the trend in the outcome before the intervention, $\\gamma_2$ is the estimation of the structural structural trend without intervention and $\\gamma_3$ is the estimation of the trend in the outcome after the intervention. We can see how the data has to be structured to be used like this in the Table 2 from the said paper just below. The analyzed event from Table 2 happened in April 2006, hence we can see this month has the value 1 for \"intervention\" in the Table. It means **the month of the event has to be included in the second period for the regression computation**.\n",
    "\n",
    ">REFERENCES : Lagarde, Mylene. “How to Do (or Not to Do) … Assessing the Impact of a Policy Change with Routine Longitudinal Data.” Health Policy and Planning 27, no. 1 (January 1, 2012): 76–83. >https://doi.org/10.1093/heapol/czr004.\n",
    "> Table 2, page 80.\n",
    "\n",
    "><img src=\"./Lagarde.png\">\n",
    "\n",
    "We have modified the technique in order to be compliant with our data since we have multiple events contrary to Lagarde who computes the regression around one unique event. The modifications are as follow :\n",
    "\n",
    ">Each event $n$ has its proper column $intervention_n$ that is filled of 1 during the months correspond to the period after the event, where it has influence on. There are then $n+1$ columns correspond to each slope between the events, before the first event and after the last event. The equation for the linear regression is : $$y_t = \\gamma_0 + \\gamma_1  \\times intervention_1 + \\dotsm + \\gamma_n  \\times intervention_n + \\gamma_{n+1}  \\times slope_1 + \\dotsm + \\gamma_{2n+1}  \\times slope_{n+1} + \\varepsilon_t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data loading and filtering\n",
    "\n",
    "> We simply load the csv's that have been extracted from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Bone</th>\n",
       "      <th>Kidney</th>\n",
       "      <th>Neurology</th>\n",
       "      <th>Quadriceps femoris muscle</th>\n",
       "      <th>Cranial nerve nucleus</th>\n",
       "      <th>Fetus</th>\n",
       "      <th>Efferent limb</th>\n",
       "      <th>Mononeuropathy multiplex</th>\n",
       "      <th>Kneecap</th>\n",
       "      <th>...</th>\n",
       "      <th>Dendrite</th>\n",
       "      <th>Hydrolysis</th>\n",
       "      <th>Murder</th>\n",
       "      <th>Autocracy</th>\n",
       "      <th>Cognition</th>\n",
       "      <th>ATP synthase</th>\n",
       "      <th>Kindness</th>\n",
       "      <th>Quantitative research</th>\n",
       "      <th>Stigmatize</th>\n",
       "      <th>House Work</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>1533</td>\n",
       "      <td>2582</td>\n",
       "      <td>1128</td>\n",
       "      <td>793</td>\n",
       "      <td>96</td>\n",
       "      <td>694</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>232</td>\n",
       "      <td>705</td>\n",
       "      <td>1804</td>\n",
       "      <td>756</td>\n",
       "      <td>1160</td>\n",
       "      <td>211</td>\n",
       "      <td>265</td>\n",
       "      <td>436</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>2058</td>\n",
       "      <td>3015</td>\n",
       "      <td>1599</td>\n",
       "      <td>1156</td>\n",
       "      <td>138</td>\n",
       "      <td>853</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>...</td>\n",
       "      <td>336</td>\n",
       "      <td>1071</td>\n",
       "      <td>2070</td>\n",
       "      <td>1018</td>\n",
       "      <td>1700</td>\n",
       "      <td>338</td>\n",
       "      <td>318</td>\n",
       "      <td>824</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>2394</td>\n",
       "      <td>3298</td>\n",
       "      <td>2000</td>\n",
       "      <td>1414</td>\n",
       "      <td>145</td>\n",
       "      <td>912</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>395</td>\n",
       "      <td>1295</td>\n",
       "      <td>2283</td>\n",
       "      <td>1090</td>\n",
       "      <td>1772</td>\n",
       "      <td>423</td>\n",
       "      <td>492</td>\n",
       "      <td>989</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>2828</td>\n",
       "      <td>3685</td>\n",
       "      <td>2035</td>\n",
       "      <td>1624</td>\n",
       "      <td>222</td>\n",
       "      <td>941</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>453</td>\n",
       "      <td>1420</td>\n",
       "      <td>2767</td>\n",
       "      <td>1005</td>\n",
       "      <td>1850</td>\n",
       "      <td>502</td>\n",
       "      <td>420</td>\n",
       "      <td>1160</td>\n",
       "      <td>6</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-05</td>\n",
       "      <td>2845</td>\n",
       "      <td>3595</td>\n",
       "      <td>2021</td>\n",
       "      <td>1579</td>\n",
       "      <td>175</td>\n",
       "      <td>952</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>482</td>\n",
       "      <td>1442</td>\n",
       "      <td>2662</td>\n",
       "      <td>1059</td>\n",
       "      <td>2003</td>\n",
       "      <td>481</td>\n",
       "      <td>426</td>\n",
       "      <td>1156</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Bone  Kidney  Neurology  Quadriceps femoris muscle  \\\n",
       "0  2017-01-01  1533    2582       1128                        793   \n",
       "1  2017-01-02  2058    3015       1599                       1156   \n",
       "2  2017-01-03  2394    3298       2000                       1414   \n",
       "3  2017-01-04  2828    3685       2035                       1624   \n",
       "4  2017-01-05  2845    3595       2021                       1579   \n",
       "\n",
       "   Cranial nerve nucleus  Fetus  Efferent limb  Mononeuropathy multiplex  \\\n",
       "0                     96    694              1                         0   \n",
       "1                    138    853              2                         1   \n",
       "2                    145    912              0                         1   \n",
       "3                    222    941              3                         1   \n",
       "4                    175    952              1                         0   \n",
       "\n",
       "   Kneecap  ...  Dendrite  Hydrolysis  Murder  Autocracy  Cognition  \\\n",
       "0       10  ...       232         705    1804        756       1160   \n",
       "1       19  ...       336        1071    2070       1018       1700   \n",
       "2       13  ...       395        1295    2283       1090       1772   \n",
       "3       12  ...       453        1420    2767       1005       1850   \n",
       "4       15  ...       482        1442    2662       1059       2003   \n",
       "\n",
       "   ATP synthase  Kindness  Quantitative research  Stigmatize  House Work  \n",
       "0           211       265                    436           4          17  \n",
       "1           338       318                    824           1          37  \n",
       "2           423       492                    989           0          25  \n",
       "3           502       420                   1160           6          27  \n",
       "4           481       426                   1156           2          24  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# # Load the climate change articles data\n",
    "# art_cc1 = pd.read_csv('data/pageviews-10-pro.csv').copy()\n",
    "# art_cc2 = pd.read_csv('data/pageviews-20-pro.csv').copy()\n",
    "# art_cc3 = pd.read_csv('data/pageviews-30-pro.csv').copy()\n",
    "# art_cc4 = pd.read_csv('data/pageviews-40-pro.csv').copy()\n",
    "# art_cc5 = pd.read_csv('data/pageviews-50-pro.csv').copy()\n",
    "\n",
    "# # Load the control (popular) articles data\n",
    "# art_control1 = pd.read_csv('data/pageviews-10-control.csv').copy()\n",
    "# art_control2 = pd.read_csv('data/pageviews-20-control.csv').copy()\n",
    "# art_control3 = pd.read_csv('data/pageviews-30-control.csv').copy()\n",
    "# art_control4 = pd.read_csv('data/pageviews-40-control.csv').copy()\n",
    "# #art_control5 = pd.read_csv('data/pageviews-50-control.csv').copy()\n",
    "\n",
    "# Load the climate change articles science data\n",
    "sci_art_cc1 = pd.read_csv('data/Pro_climate/pageviews-group1-biodiv.csv').copy()\n",
    "sci_art_cc2 = pd.read_csv('data/Pro_climate/pageviews-group1-energy.csv').copy()\n",
    "sci_art_cc3 = pd.read_csv('data/Pro_climate/pageviews-group1-meteo.csv').copy()\n",
    "sci_art_cc4 = pd.read_csv('data/Pro_climate/pageviews-group1-pesticides.csv').copy()\n",
    "sci_art_cc5 = pd.read_csv('data/Pro_climate/pageviews-groupe1-earth.csv').copy()\n",
    "\n",
    "# Load the control articles science data\n",
    "sci_art_control1 = pd.read_csv('data/Group_control/pageviews-group2-anatomy.csv').copy()\n",
    "sci_art_control2 = pd.read_csv('data/Group_control/pageviews-group2-genetics.csv').copy()\n",
    "sci_art_control3 = pd.read_csv('data/Group_control/pageviews-group2-optic.csv').copy()\n",
    "sci_art_control4 = pd.read_csv('data/Group_control/pageviews-group2-philo.csv').copy()\n",
    "sci_art_control5 = pd.read_csv('data/Group_control/pageviews-group2-social.csv').copy()\n",
    "\n",
    "# Load the control articles science data\n",
    "sci_art_pop1 = pd.read_csv('data/Group_popular_2017/pageviews-control-2017.csv').copy()\n",
    "sci_art_pop2 = pd.read_csv('data/Group_popular_2017/pageviews-control-2017-1.csv').copy()\n",
    "sci_art_pop3 = pd.read_csv('data/Group_popular_2017/pageviews-control-2017-2.csv').copy()\n",
    "sci_art_pop4 = pd.read_csv('data/Group_popular_2017/pageviews-control-2017-3.csv').copy()\n",
    "sci_art_pop5 = pd.read_csv('data/Group_popular_2017/pageviews-control-2017-4.csv').copy()\n",
    "\n",
    "# Merge all datasets together for each corpus\n",
    "# art_cc=art_cc1.merge(art_cc2).merge(art_cc3).merge(art_cc4).merge(art_cc5)\n",
    "# art_control_popular=art_control1.merge(art_control2).merge(art_control3).merge(art_control4)#.merge(art_control5)\n",
    "sci_art_cc=sci_art_cc1.merge(sci_art_cc2).merge(sci_art_cc3).merge(sci_art_cc4).merge(sci_art_cc5)\n",
    "sci_art_control=sci_art_control1.merge(sci_art_control2).merge(sci_art_control3).merge(sci_art_control4).merge(sci_art_control5)\n",
    "sci_art_pop=sci_art_pop1.merge(sci_art_pop2).merge(sci_art_pop3).merge(sci_art_pop4).merge(sci_art_pop5)\n",
    "# Display to illustrate the structure\n",
    "sci_art_control.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Here we define all the intervals between the mentioned event about the climate : the school strike of August 20, 2018 led by Greta Thunberg, the 2018 United Nations Climate Change Conference (COP24) on December 14, 2018 and the summit of the United Nations on September 23, 2019. The modularity of the code allows to simply add or put out an event without changing the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTERVALS_2 =[[\"2018-01\",\"2018-07\"],[\"2018-08\",\"2018-11\"],[\"2018-12\",\"2019-08\"],[\"2019-09\",\"2020-02\"]]\n",
    "#1er janvier 2017 jusquau 31 aout 2020\n",
    "INTERVALS =[(\"2017-01\",\"2018-07\"),(\"2018-08\",\"2018-11\"),(\"2018-12\",\"2019-08\"),(\"2019-09\",\"2020-08\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Here we have a function to aggregate the dataframe. It creates a dataframe with the index as the datetimes by month and with two columns as the number of the month and the pageviews count over all the article during the said month. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pageviews</th>\n",
       "      <th>months</th>\n",
       "      <th>intervention_1</th>\n",
       "      <th>intervention_2</th>\n",
       "      <th>intervention_3</th>\n",
       "      <th>slope_1</th>\n",
       "      <th>slope_2</th>\n",
       "      <th>slope_3</th>\n",
       "      <th>slope_4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-01-01</th>\n",
       "      <td>0.978295</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-01</th>\n",
       "      <td>0.919070</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-01</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-04-01</th>\n",
       "      <td>0.897990</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-01</th>\n",
       "      <td>0.940889</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-01</th>\n",
       "      <td>0.893121</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-01</th>\n",
       "      <td>0.832797</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-01</th>\n",
       "      <td>0.822961</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-01</th>\n",
       "      <td>0.934926</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-01</th>\n",
       "      <td>0.976179</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-01</th>\n",
       "      <td>0.951158</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-01</th>\n",
       "      <td>0.816359</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01</th>\n",
       "      <td>0.935198</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-01</th>\n",
       "      <td>0.873926</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-01</th>\n",
       "      <td>0.949391</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-01</th>\n",
       "      <td>0.910087</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-01</th>\n",
       "      <td>0.925170</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-01</th>\n",
       "      <td>0.840382</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-01</th>\n",
       "      <td>0.863914</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-01</th>\n",
       "      <td>0.934439</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-01</th>\n",
       "      <td>0.910984</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-01</th>\n",
       "      <td>0.923918</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-01</th>\n",
       "      <td>0.936638</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-01</th>\n",
       "      <td>0.827620</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01</th>\n",
       "      <td>0.888930</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-01</th>\n",
       "      <td>0.792703</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-01</th>\n",
       "      <td>0.870088</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-01</th>\n",
       "      <td>0.835867</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-01</th>\n",
       "      <td>0.886612</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-01</th>\n",
       "      <td>0.791138</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-01</th>\n",
       "      <td>0.751661</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-01</th>\n",
       "      <td>0.755974</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-01</th>\n",
       "      <td>0.851531</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-01</th>\n",
       "      <td>0.852373</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-01</th>\n",
       "      <td>0.845614</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-01</th>\n",
       "      <td>0.726675</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01</th>\n",
       "      <td>0.836485</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-01</th>\n",
       "      <td>0.778013</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01</th>\n",
       "      <td>0.753580</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-01</th>\n",
       "      <td>0.842035</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-01</th>\n",
       "      <td>0.753214</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-01</th>\n",
       "      <td>0.627070</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-01</th>\n",
       "      <td>0.683943</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-01</th>\n",
       "      <td>0.644699</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            pageviews  months  intervention_1  intervention_2  intervention_3  \\\n",
       "Date                                                                            \n",
       "2017-01-01   0.978295       1               0               0               0   \n",
       "2017-02-01   0.919070       2               0               0               0   \n",
       "2017-03-01   1.000000       3               0               0               0   \n",
       "2017-04-01   0.897990       4               0               0               0   \n",
       "2017-05-01   0.940889       5               0               0               0   \n",
       "2017-06-01   0.893121       6               0               0               0   \n",
       "2017-07-01   0.832797       7               0               0               0   \n",
       "2017-08-01   0.822961       8               0               0               0   \n",
       "2017-09-01   0.934926       9               0               0               0   \n",
       "2017-10-01   0.976179      10               0               0               0   \n",
       "2017-11-01   0.951158      11               0               0               0   \n",
       "2017-12-01   0.816359      12               0               0               0   \n",
       "2018-01-01   0.935198      13               0               0               0   \n",
       "2018-02-01   0.873926      14               0               0               0   \n",
       "2018-03-01   0.949391      15               0               0               0   \n",
       "2018-04-01   0.910087      16               0               0               0   \n",
       "2018-05-01   0.925170      17               0               0               0   \n",
       "2018-06-01   0.840382      18               0               0               0   \n",
       "2018-07-01   0.863914      19               0               0               0   \n",
       "2018-08-01   0.934439      20               1               0               0   \n",
       "2018-09-01   0.910984      21               1               0               0   \n",
       "2018-10-01   0.923918      22               1               0               0   \n",
       "2018-11-01   0.936638      23               1               0               0   \n",
       "2018-12-01   0.827620      24               0               1               0   \n",
       "2019-01-01   0.888930      25               0               1               0   \n",
       "2019-02-01   0.792703      26               0               1               0   \n",
       "2019-03-01   0.870088      27               0               1               0   \n",
       "2019-04-01   0.835867      28               0               1               0   \n",
       "2019-05-01   0.886612      29               0               1               0   \n",
       "2019-06-01   0.791138      30               0               1               0   \n",
       "2019-07-01   0.751661      31               0               1               0   \n",
       "2019-08-01   0.755974      32               0               1               0   \n",
       "2019-09-01   0.851531      33               0               0               1   \n",
       "2019-10-01   0.852373      34               0               0               1   \n",
       "2019-11-01   0.845614      35               0               0               1   \n",
       "2019-12-01   0.726675      36               0               0               1   \n",
       "2020-01-01   0.836485      37               0               0               1   \n",
       "2020-02-01   0.778013      38               0               0               1   \n",
       "2020-03-01   0.753580      39               0               0               1   \n",
       "2020-04-01   0.842035      40               0               0               1   \n",
       "2020-05-01   0.753214      41               0               0               1   \n",
       "2020-06-01   0.627070      42               0               0               1   \n",
       "2020-07-01   0.683943      43               0               0               1   \n",
       "2020-08-01   0.644699      44               0               0               1   \n",
       "\n",
       "            slope_1  slope_2  slope_3  slope_4  \n",
       "Date                                            \n",
       "2017-01-01        1        0        0        0  \n",
       "2017-02-01        2        0        0        0  \n",
       "2017-03-01        3        0        0        0  \n",
       "2017-04-01        4        0        0        0  \n",
       "2017-05-01        5        0        0        0  \n",
       "2017-06-01        6        0        0        0  \n",
       "2017-07-01        7        0        0        0  \n",
       "2017-08-01        8        0        0        0  \n",
       "2017-09-01        9        0        0        0  \n",
       "2017-10-01       10        0        0        0  \n",
       "2017-11-01       11        0        0        0  \n",
       "2017-12-01       12        0        0        0  \n",
       "2018-01-01       13        0        0        0  \n",
       "2018-02-01       14        0        0        0  \n",
       "2018-03-01       15        0        0        0  \n",
       "2018-04-01       16        0        0        0  \n",
       "2018-05-01       17        0        0        0  \n",
       "2018-06-01       18        0        0        0  \n",
       "2018-07-01       19        0        0        0  \n",
       "2018-08-01       19        1        0        0  \n",
       "2018-09-01       19        2        0        0  \n",
       "2018-10-01       19        3        0        0  \n",
       "2018-11-01       19        4        0        0  \n",
       "2018-12-01       19        4        1        0  \n",
       "2019-01-01       19        4        2        0  \n",
       "2019-02-01       19        4        3        0  \n",
       "2019-03-01       19        4        4        0  \n",
       "2019-04-01       19        4        5        0  \n",
       "2019-05-01       19        4        6        0  \n",
       "2019-06-01       19        4        7        0  \n",
       "2019-07-01       19        4        8        0  \n",
       "2019-08-01       19        4        9        0  \n",
       "2019-09-01       19        4        9        1  \n",
       "2019-10-01       19        4        9        2  \n",
       "2019-11-01       19        4        9        3  \n",
       "2019-12-01       19        4        9        4  \n",
       "2020-01-01       19        4        9        5  \n",
       "2020-02-01       19        4        9        6  \n",
       "2020-03-01       19        4        9        7  \n",
       "2020-04-01       19        4        9        8  \n",
       "2020-05-01       19        4        9        9  \n",
       "2020-06-01       19        4        9       10  \n",
       "2020-07-01       19        4        9       11  \n",
       "2020-08-01       19        4        9       12  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def aggregate(df, freq=\"MS\", drop = None, events=INTERVALS, normalization=False) :\n",
    "    \"\"\"A function to aggregate the dataframe as a Serie sorted by month\n",
    "    with the total number of pageviews per month. With the numbering of each\n",
    "    month of the timeserie. drop is used to drop a list of articles in the \n",
    "    dataframe. freq is used to choice the frequency of the aggregation. \n",
    "    Add the columns intervention, preslope and postslope in the Lagarde fashion.\n",
    "    To perform the aggregation, it is necessary to give the date of the analyzed \n",
    "    event according to the chosen frequency.\"\"\"\n",
    "    df=df.copy()\n",
    "    df.index=df.Date\n",
    "    df.index=pd.to_datetime(df.index)\n",
    "#     df.index=df.index.ceil(freq) #round below according to a frequency\n",
    "    df=df.drop(columns=\"Date\")\n",
    "    #Recreate the dataframe without the named articles\n",
    "    if not drop == None :\n",
    "        df=df.drop(columns=drop)\n",
    "    \n",
    "    # At this point, we should  sum over all articles.\n",
    "    df=df.sum(axis=1)\n",
    "    # Agregate the data by months\n",
    "    df=df.groupby(pd.Grouper(freq=freq)).sum()\n",
    "    #Serie to dataframe\n",
    "    df=df.to_frame()\n",
    "    #Rename the column\n",
    "    df.columns=['pageviews']\n",
    "    #Add the column months to help to plot with the regression\n",
    "    months=np.arange(1, len(df.index)+1)\n",
    "    df[\"months\"]=months\n",
    "    #Create a column per event as intervention_i with \"1\" values during the timedelta of the event\n",
    "    for inters, nb in zip(events, range(0, len(events))) :\n",
    "        #Pass the first interval since we are not \"after\" a event\n",
    "        if inters == events[0] :\n",
    "            pass\n",
    "        else :\n",
    "            #Add the intervention column (0 before the event)\n",
    "            df[f\"intervention_{nb}\"]=[0 if (pd.Timestamp(inters[0])> month or month > pd.Timestamp(inters[1])) else 1 for month in df.index]\n",
    "    #Modular creation of the slope array. Consider a slope between two event, the values before\n",
    "    #the first event is 0 and are equal the month of the second event -1 after the second event as Lagarde's\n",
    "    for event_int, n_slope in zip(events, range(1, len(events)+1)) :\n",
    "        df[f\"slope_{n_slope}\"]=list(np.zeros(len(df), dtype=int))\n",
    "        idx_event0=df.at[event_int[0], \"months\"]\n",
    "        idx_event1=df.at[event_int[1], \"months\"]\n",
    "        #Fill with zero and start counting from the first event\n",
    "        df[f\"slope_{n_slope}\"]=df.apply(lambda x : int(x[\"months\"]-(idx_event0-1)) if x[\"months\"]>=idx_event0 else 0, axis=1)\n",
    "        #Mask all the values after the second event by the month of the event minus one\n",
    "        df[f\"slope_{n_slope}\"]=df.apply(lambda x : int(idx_event1-idx_event0+1) if x[\"months\"]>=idx_event1 else x[f\"slope_{n_slope}\"], axis=1)\n",
    "    #We just need to add one to the last value because we have defined the last entry of the datetime index of the dataframe as an event \n",
    "    #(e.g. the last value are duplicate [14, 14] and we want [14, 15])\n",
    "    df[f\"slope_{len(events)}\"].iat[-1]=df[f\"slope_{len(events)}\"].iat[-2]+1 \n",
    "    \n",
    "    # Apply normalization (min-max)\n",
    "    if normalization :\n",
    "        max_value = df[\"pageviews\"].max()\n",
    "        df[\"pageviews\"]=df[\"pageviews\"].apply(lambda x: x/max_value)\n",
    "\n",
    "    return df\n",
    "\n",
    "sci_art_cc_agg=aggregate(df=sci_art_cc, normalization=True)\n",
    "sci_art_control_agg=aggregate(df=sci_art_control, normalization=True)\n",
    "sci_art_pop_agg=aggregate(df=sci_art_pop, normalization=True)\n",
    "\n",
    "display(sci_art_cc_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2 Implementing the methods\n",
    "### 2.1 Computing the non model empirical comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">First, a very simple analysis is performed by doing the average monthly pageview count for each period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'art_cc_agg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-0c500111acc0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mart_cc_monthly_mean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mperiods_avg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mart_cc_agg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mINTERVALS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mart_control_popular_monthly_mean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mperiods_avg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mart_control_popular_agg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mINTERVALS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'art_cc_agg' is not defined"
     ]
    }
   ],
   "source": [
    "# Compute the mean view count per months for the different periods\n",
    "def periods_avg(df, intervals) :\n",
    "    monthly_mean = pd.DataFrame(data={'monthly_mean': [0] * len(INTERVALS)})\n",
    "    count=0\n",
    "    \n",
    "    for interval in intervals :\n",
    "        sub_df=df.loc[interval[0]:interval[1]]\n",
    "        mean=sub_df['pageviews'].mean()\n",
    "        monthly_mean.loc[count, 'monthly_mean'] = mean\n",
    "        count=count+1\n",
    "    \n",
    "    return monthly_mean\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sci_art_cc_monthly_mean = periods_avg(sci_art_cc_agg, INTERVALS)\n",
    "sci_art_control_monthly_mean = periods_avg(sci_art_control_agg, INTERVALS)\n",
    "sci_art_pop_monthly_mean = periods_avg(sci_art_pop_agg, INTERVALS)\n",
    "\n",
    "art_control_popular_monthly_mean\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Here we define a function to compute multiple linear regression according to intervals given as arguments. The function returns a dict with the summary of the results, the predicted values corresponding to the given pageviews/month dataframe and the upper/lower values of the confidence intervals for each regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2.2 Computing the Segmented Linear Regression parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Here we have a function to plot two dataframes, generally the studied group of Wikipedia articles and a selected comparator group. Each event is display with a vertical bar to separator each period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "import datetime\n",
    "from statsmodels.stats.outliers_influence import summary_table\n",
    "\n",
    "def get_intervals_from_lagarde(df) :\n",
    "    \"\"\"Allow to obtain the list of intervals between events as tuples of strings from the dataframe\n",
    "    in Lagarde fashion. \"\"\"\n",
    "    \n",
    "    #Obtain the intervals strings between the event.\n",
    "    #We want pairs of date with an offset of one between events so\n",
    "    #we create 2 lists starting with one event lag and zip the two ones\n",
    "    interval_early=[df.index[0].strftime('%Y-%m')]\\\n",
    "        +[df[df[column]==1].index[0].strftime('%Y-%m') for column in df.columns if column.startswith('intervention')] #[df[df[column]==1].index[0] = the first date beginning with 1 in the intervention column = the event\n",
    "    #Here it is complicated, we need to take the index of the first value \"1\" and substract\n",
    "    #one month to obtain the index correspond to the second element of the interval\n",
    "    interval_late=[(df[df[column]==1].index[0]-pd.DateOffset(months=1)).strftime('%Y-%m') for column in df.columns if column.startswith('intervention')]\\\n",
    "        +[df.index[-1].strftime('%Y-%m')] \n",
    "    return list(zip(interval_early, interval_late))\n",
    "    \n",
    "def lin_reg_period_lagarde(df, params={},CI=0.05) :\n",
    "    \"\"\"Function that returns the results of the OLS linear regression for the data points\n",
    "    around a particular event. The provided dataset must include one column 'intervention' \n",
    "    per event and a column per slope for each slope according to the Lagarde's format of\n",
    "    Interrupted Time Series. The value of each key\n",
    "    is a another dictionnary containing the results (summary), the array of predicted values\n",
    "    and the upper and lower values for the confidence interval. It is possible to specify the interval \n",
    "    of each period with before_interval and after_interval arguments as a list in params argument. \n",
    "    To give customized interval ->  params = {interval_number : ['2012-01','2013-05'], ...}\"\"\"\n",
    "\n",
    "    df=df.copy()\n",
    "    \n",
    "    #Get the list of intervals\n",
    "    intervals = get_intervals_from_lagarde(df)\n",
    "    #Check for customized interval\n",
    "    for interval_number in range(1,len(intervals)+1) :\n",
    "        intervals[interval_number-1]=params[interval_number] if interval_number in params.keys() else intervals[interval_number-1]   \n",
    "\n",
    "    #Create the formula string (pageviews must be the first column)\n",
    "    formula=f\"{df.columns[0]} ~ \" + \" + \".join([column for column in df.columns[2:]])\n",
    "    #Linear regression with pageviews as dependent variable and other columns as indepedent variables as Lagarde.\n",
    "    #Make the regression only on the custom provided intervals \n",
    "    mod = smf.ols(formula=formula, data=df[intervals[0][0]:intervals[-1][1]])\n",
    "    res=mod.fit()\n",
    "    #Will store the data points separately for each period\n",
    "    results_dict={\"results\" : res}\n",
    "    #iterate through the intervals to compute their regression.\n",
    "    #Create the cumulative intercept value as depicted below\n",
    "    cumulative_intercept=res.params[0]\n",
    "    for interval, number_int in zip(intervals, list(range(1,len(intervals)+1,1))) :\n",
    "        #Extract the computed values (including upper and lower CI values)\n",
    "        _, summary_values, summary_names = summary_table(res, alpha=CI)\n",
    "        #Create a temporary dataframe with the result\n",
    "        df_res_tmp = pd.DataFrame(summary_values, columns=summary_names)\n",
    "#         display(df_res_tmp)  #-> To see how the data is structured\n",
    "        \n",
    "        #Extract the predicted values according to one period to plot them later \n",
    "        pages_predict=df_res_tmp.iloc[df[interval[0]][\"months\"][0]-1:df[interval[1]][\"months\"][0],:][\"Predicted\\nValue\"].T\n",
    "        #WARNING : It's a bit tricky but the column has always 95% in its name even if the CI\n",
    "        #introduce in alpha is different of 0.05. It has been tested and the values are different.\n",
    "        #Here we are taking only the values corresponding to the predictions of the current slope\n",
    "        predict_ci_low = df_res_tmp.iloc[df[interval[0]][\"months\"][0]-1:df[interval[1]][\"months\"][0],:][\"Mean ci\\n95% low\"].T\n",
    "        predict_ci_upp = df_res_tmp.iloc[df[interval[0]][\"months\"][0]-1:df[interval[1]][\"months\"][0],:][\"Mean ci\\n95% upp\"].T\n",
    "        \n",
    "        result={\n",
    "            \"predicts\" : pages_predict,\n",
    "            \"CI\" : {\n",
    "                \"lower\" : predict_ci_low,\n",
    "                \"upper\" : predict_ci_upp\n",
    "            }\n",
    "        }\n",
    "        results_dict[number_int]=result\n",
    "    return results_dict\n",
    "                                    \n",
    "\n",
    "        \n",
    "#       Create a dictionnary to pass the results. (See the structure below)\n",
    "\n",
    " \n",
    "    #Create a dictionnary to pass the results. The structure is as follow :\n",
    "#     dic = {\n",
    "#         \"result\" : res_before,\n",
    "#         \"interval1\" :{\n",
    "#             \"predicts\" : pages_predict_before,\n",
    "#             \"CI\" : {\n",
    "#                 \"lower\" : predict_ci_low_before,\n",
    "#                 \"upper\" : predict_ci_upp_before\n",
    "#             }\n",
    "#         }, \n",
    "#         \"interval2\" : {\n",
    "#             \"predicts\" : pages_predict_after,\n",
    "#             \"CI\" : {\n",
    "#                 \"lower\" : predict_ci_low_after,\n",
    "#                 \"upper\" : predict_ci_upp_after\n",
    "#             }\n",
    "#         },\n",
    "#         ...\n",
    "#     }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">We can try the function. The regressions are computed and will be used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "sci_art_control_reg=lin_reg_period_lagarde(df=sci_art_control_agg)\n",
    "sci_art_cc_reg=lin_reg_period_lagarde(df=sci_art_cc_agg);\n",
    "sci_art_pop_reg=lin_reg_period_lagarde(df=sci_art_pop_agg);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Displaying the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Non model empirical - Mean comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import (AutoMinorLocator, MultipleLocator)\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "def plot_means(dfs, params)  :\n",
    "    \"\"\"Function that plots the data to compare 2 ITS. It must be given the results of \n",
    "    the linear regression and the timeserie with the numbering by month. As the linear\n",
    "    regression function, it is possible to specify the interval of months for each period.\n",
    "    All the params are given through dictionnary. The dataframes are given as a tuple and the\n",
    "    corresponding results of the linear regression as well. The confidence area for each linear\n",
    "    regression is plotted as an area.\n",
    "    As follow : (df_studied, df_comparator)\"\"\"\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~ Extract parameter ~~~~~~~~~~~~~~~~~~~~~~\n",
    "    keys=params.keys()\n",
    "    title=params[\"title\"] if \"title\" in keys else \"\"\n",
    "    xlabel=params[\"xlabel\"] if \"xlabel\" in keys else \"\"\n",
    "    ylabel=params[\"ylabel\"] if \"ylabel\" in keys else \"\"\n",
    "    filename=params[\"filename\"] if \"filename\" in keys else \"no_name\"\n",
    "    #Names used to add in the legend\n",
    "    names=params[\"names\"] if \"names\" in keys else [\"\", \"\"]\n",
    "    intervals=params[\"intervals\"] if \"intervals\" in keys else []\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~ Create the empty plot ~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    fig, ax = plt.subplots(1,1,figsize= (7,7), sharey = True, sharex = True)\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~ Plotting all the data points of the serie ~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    #Plots the two plots, the second plot is the comparator group\n",
    "\n",
    "    ax.scatter(dfs[0].index, dfs[0], marker=\"o\", s=200, c='red', label=\"Climate change related articles\")\n",
    "    ax.scatter(dfs[1].index, dfs[1], marker=\"o\", s=200, c='black', label=\"Popular Articles\")\n",
    "\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~ Visual modifications of the plot ~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    ax.set_xlabel(xlabel, fontsize=12)\n",
    "    ax.set_ylabel(ylabel, fontsize=12)\n",
    "    ax.xaxis.grid(False)\n",
    "    #Plot tick bars\n",
    "    #Plot a vertical line to delimit the 2 intervals.\n",
    "    #-0.5 to locate the mid of the month. (Since the frequency is end of month)\n",
    "    \n",
    "    #Use to avoid \"duplicates\" in the legend\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    plt.legend(by_label.values(), by_label.keys(), loc='upper left',fontsize=12,bbox_to_anchor=(0.135, -0.15),ncol=2,frameon=True,edgecolor=\"black\")\n",
    "    fig.tight_layout()\n",
    "    fig.suptitle(title, y=1.07, fontsize=20)\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~ Save the plot in order to plot it further ~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    plt.savefig(f\"./plots/{filename}\", format=\"png\",bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.1.1 Climate change articles VS Quasi-control group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "    \"title\" : \"Climate articles vs. Comparator articles (Quasi-control)\",\n",
    "    \"xlabel\" : \"Different periods\",\n",
    "    \"ylabel\" : \"Monthly mean of pageviews\",\n",
    "    \"filename\" : \"climate_vs_quasicontrol_MEAN.png\",\n",
    "    \"intervals\" : INTERVALS\n",
    "}\n",
    "dfs=(sci_art_cc_monthly_mean, sci_art_control_monthly_mean)\n",
    "plot_means(dfs=dfs, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.1.2 Climate change articles VS Popular comparator group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "    \"title\" : \"Climate articles vs. Popular articles 2017, 2018, 2019, 2020\",\n",
    "    \"xlabel\" : \"Different periods\",\n",
    "    \"ylabel\" : \"Monthly mean of pageviews\",\n",
    "    \"filename\" : \"climate_vs_pop_MEAN.png\",\n",
    "    \"intervals\" : INTERVALS\n",
    "}\n",
    "dfs=(sci_art_cc_monthly_mean, sci_art_pop_monthly_mean)\n",
    "plot_means(dfs=dfs, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3.2 Interrupted Time Serie (ITS) - Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Here we're defining the function to plot the ITS regression over months for 2 groups. The chosen events are displayed as vertical lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparator(dfs, lin_reg_res, params)  :\n",
    "    \"\"\"Function that plots the data to compare 2 ITS. It must be given the results of \n",
    "    the linear regression and the timeserie with the numbering by month. As the linear\n",
    "    regression function, it is possible to specify the interval of months for each period.\n",
    "    All the params are given through dictionnary. The dataframes are given as a tuple and the\n",
    "    corresponding results of the linear regression as well. The confidence area for each linear\n",
    "    regression is plotted as an area.\n",
    "    As follow : (df_studied, df_comparator)\"\"\"\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~ Extract parameter ~~~~~~~~~~~~~~~~~~~~~~\n",
    "    keys=params.keys()\n",
    "    title=params[\"title\"] if \"title\" in keys else \"\"\n",
    "    xlabel=params[\"xlabel\"] if \"xlabel\" in keys else \"\"\n",
    "    ylabel=params[\"ylabel\"] if \"ylabel\" in keys else \"\"\n",
    "    filename=params[\"filename\"] if \"filename\" in keys else \"no_name\"\n",
    "    #Names used to add in the legend\n",
    "    names=params[\"names\"] if \"names\" in keys else [\"\", \"\"]\n",
    "    ci=params[\"ci\"] if \"ci\" in keys else \"\"\n",
    "    #axis limits\n",
    "    TOP=params[\"top\"] if \"top\" in keys else None\n",
    "    BOTTOM=params[\"bottom\"] if \"bottom\" in keys else None\n",
    "    \n",
    "        \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~ Custom intervals ~~~~~~~~~~~~~~~~~~~~~~\n",
    "    #Get the list of intervals\n",
    "    intervals = get_intervals_from_lagarde(dfs[0])\n",
    "    \n",
    "    #Check for customized interval\n",
    "    for interval_number in range(1,len(intervals)+1) :\n",
    "        if \"custom_intervals\" in keys :\n",
    "            intervals[interval_number-1]=params[\"custom_intervals\"][interval_number] if interval_number in params[\"custom_intervals\"].keys() else intervals[interval_number-1]\n",
    "    \n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~ Fix limit of the axes ~~~~~~~~~~~~~~~~~~~~~~ \n",
    "    \n",
    "    #Boundaries of the y-axis, it has been decided to keep them constant instead of relative by % of the\n",
    "    #max and min value because it is easier to compare plot when the axes are constant\n",
    "    BOTTOM=0 if BOTTOM is None else BOTTOM\n",
    "    all_pageviews=[j for i in [list(dfs[0][\"pageviews\"]), list(dfs[1][\"pageviews\"])] for j in i]\n",
    "    MAX=max(all_pageviews)\n",
    "    TOP=MAX+0.1*MAX if TOP is None else TOP#1500000#100000000 \n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~ Create the empty plot ~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    fig, ax = plt.subplots(1,1,figsize= (11,7), sharey = True, sharex = True)\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~ Plotting all the data points of the serie ~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    #Choose in advance the colors of the two groups\n",
    "#     colors=[(\"black\", \"dimgray\"), (\"darkgrey\", \"silver\")]\n",
    "    colors=[(\"darkred\", \"firebrick\", \"indianred\"), (\"mediumaquamarine\", \"aquamarine\", \"paleturquoise\")]\n",
    "    #Plots the two plots, the second plot is the comparator group\n",
    "    for df, reg, col, name in zip(dfs, lin_reg_res, colors, names) :\n",
    "        #Just to have the correct keys for the reg dict and select\n",
    "        #properly the right period\n",
    "        name_interval=[\"-\".join(inter) for inter in intervals]\n",
    "        \n",
    "        ax.scatter(x=df[\"months\"],y=df[\"pageviews\"], marker=\"o\", s=50, c=col[0], label=f\"{name}-related Articles\")\n",
    "        #Plotting linear regression\n",
    "        for interval, n_inter, key in zip(intervals, name_interval,  list(range(1,len(intervals)+1,1))) :\n",
    "            ax.plot(df.loc[interval[0]:interval[1]][\"months\"],\n",
    "                    reg[key][\"predicts\"], #Take the data points of the regression from the dict\n",
    "                    linewidth=4,\n",
    "                    alpha=0.9,\n",
    "                    c=col[1],\n",
    "                    label=f\"{name} Article Trend\") #{n_inter}\n",
    "        \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~ CI plot ~~~~~~~~~~~~~~~~~~~~~~\n",
    "        #For each regression line, plot the upper CI values as a line and\n",
    "        #the lower CI values. Filled the area between the two.\n",
    "        periods = list(reg.values())[1:]\n",
    "        for period, interval in zip(periods, intervals) :\n",
    "            for _, ci_array in period[\"CI\"].items() :\n",
    "                ax.plot(df.loc[interval[0]:interval[1]][\"months\"],\n",
    "                        ci_array,\n",
    "                        linewidth=2,\n",
    "                        alpha=0.1,\n",
    "                        c=col[2]\n",
    "                       )\n",
    "            #Create a color area representing the CI\n",
    "            plt.fill_between(df.loc[interval[0]:interval[1]][\"months\"],\n",
    "                period[\"CI\"][\"lower\"],\n",
    "                period[\"CI\"][\"upper\"],\n",
    "                where=period[\"CI\"][\"upper\"] >= period[\"CI\"][\"lower\"],\n",
    "                facecolor=col[2], alpha=0.25, interpolate=True, label=f\"{name} Confidence Interval ({ci})\")\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~ Visual modifications of the plot ~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    ax.set_ylim(bottom=BOTTOM, top=TOP)\n",
    "    ax.set_xlabel(xlabel, fontsize=12)\n",
    "    ax.set_ylabel(ylabel, fontsize=12)\n",
    "    ax.xaxis.grid(False)\n",
    "    ax.xaxis.set_major_locator(MultipleLocator(2))\n",
    "    ax.xaxis.set_minor_locator(AutoMinorLocator(4))\n",
    "    ax.xaxis.set_tick_params(length = 5, width = 1)\n",
    "    ax.yaxis.set_tick_params(length = 5, width = 1)\n",
    "    #Plot tick bars\n",
    "    ax.ticklabel_format(useOffset=False, style='plain')\n",
    "    #Plot a vertical line to delimit the 2 intervals.\n",
    "    #-0.5 to locate the mid of the month. (Since the frequency is end of month)\n",
    "    for interval, counter in zip(intervals, list(range(1,len(intervals)+1,1)))  :\n",
    "        #The event is located always as the first month of an interval except the first one\n",
    "        if counter > 1 :\n",
    "            event=float(df.loc[interval[0]][\"months\"]-0.5)\n",
    "            plt.axvline(x=event, color = 'black', alpha=1, linewidth=3.5)\n",
    "            #Indicate the mid June (Arbitrary addition values to center the label)\n",
    "            ax.text(event-1.1, TOP, interval[0], fontsize=13)\n",
    "    #Use to avoid \"duplicates\" in the legend\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    plt.legend(by_label.values(), by_label.keys(), loc='upper left',fontsize=12,bbox_to_anchor=(0.135, -0.15),ncol=2,frameon=True,edgecolor=\"black\")\n",
    "    fig.tight_layout()\n",
    "    fig.suptitle(title, y=1.07, fontsize=20)\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~ Save the plot in order to plot it further ~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    plt.savefig(f\"./plots/{filename}\", format=\"png\",bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.2.1 Climate change articles VS Quasi-control group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the function \n",
    "params={\n",
    "    \"title\" : \"Climate articles vs. Comparator articles (Quasi-control)\",\n",
    "    \"xlabel\" : \"Time (Months)\",\n",
    "    \"ylabel\" : \"Total Views (All Articles)\",\n",
    "    \"filename\" : \"climate_vs_quasicontrol.png\",\n",
    "    \"ci\" : \"95%\",\n",
    "    \"names\" : (\"Climate\",\"Quasi-control\"),\n",
    "    \"intervals\" : INTERVALS,\n",
    "    \"bottom\" : 0.18,\n",
    "    \"top\" : 1.1\n",
    "}\n",
    "dfs=(sci_art_cc_agg, sci_art_control_agg)\n",
    "regs=(sci_art_cc_reg, sci_art_control_reg)\n",
    "plot_comparator(dfs=dfs, lin_reg_res=regs, params=params)\n",
    "\n",
    "print(f\"CC | Linear Regression interval 1 :\\n\\n {sci_art_cc_reg['results'].summary()}\\n\\n\")\n",
    "print(f\"QUasi-control | Linear Regression interval 1 :\\n\\n {sci_art_control_reg['results'].summary()}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">In the publication, the terrorism articles are compared to another quasi-control group. This group is made or security related articles that would arguably not be affected by the treatment but are similar enough in terms of topic to be used as a control group. The following cells present an attempt at the replication of that method.\n",
    "\n",
    "> To be able to reproduce this method in our context, we need a group of articles that are likely to be affected by the treatment, namely that people would consult them more because of the mediatic movement around Greta Thunberg and climate change. The second group should be a group of articles that are similar to the first group such that any external bias would appear in both and therefore cancel out at comparison. It should also not be affected by the treatment, namely not be related to climate change issues. It is not straight forward to find a second group that would be used along with the first corpus that we considered, the corpus of the climate change related articles plotted above because this group is already quite broad. \n",
    "\n",
    "> To paliate to this problem, we restrained ourselves to scientific articles chosen in wikipedia index of articles randomly. The first group was selected randomly in a list of scientific articles on topics likely to be related to climate change (biodiversity, energy, meteo, pesticides, earth). The second group was selected randomly in a list of scientific articles on topics not related to climate change (anatomy, genetics, optics, philosophy, social). This pick has the advantage of showing if the public reads on scientific articles, which would confirm the hypothesis that people were encouraged to get educated on the topic of climate change. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.2.2 Climate change articles VS Popular comparator group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the function \n",
    "params={\n",
    "    \"title\" : \"Climate articles vs. Popular articles 2017, 2018, 2019, 2020 (Quasi-control)\",\n",
    "    \"xlabel\" : \"Time (Months)\",\n",
    "    \"ylabel\" : \"Total Views (All Articles)\",\n",
    "    \"filename\" : \"climate_vs_pop.png\",\n",
    "    \"ci\" : \"95%\",\n",
    "    \"names\" : (\"Climate\",\"Quasi-control\"),\n",
    "    \"intervals\" : INTERVALS,\n",
    "    \"bottom\" : 0.18,\n",
    "    \"top\" : 1.1\n",
    "}\n",
    "dfs=(sci_art_cc_agg, sci_art_pop_agg)\n",
    "regs=(sci_art_cc_reg, sci_art_pop_reg)\n",
    "plot_comparator(dfs=dfs, lin_reg_res=regs, params=params)\n",
    "\n",
    "print(f\"CC | Linear Regression interval 1 :\\n\\n {sci_art_cc_reg['results'].summary()}\\n\\n\")\n",
    "print(f\"QUasi-control | Linear Regression interval 1 :\\n\\n {sci_art_pop_reg['results'].summary()}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ada]",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
