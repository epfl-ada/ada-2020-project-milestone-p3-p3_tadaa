{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project milestone 4\n",
    "Tadaa - Jonathan Haenni, Lea Schmidt, Danny Kohler\n",
    "> This work presents the creative extension of the publication from J. Penney : \"Chilling effects : Online surveillance and Wikipedia use\".\n",
    "\n",
    "---\n",
    "\n",
    "## Abstract\n",
    "The goal of this work is to observe if the public action of Greta Thunberg and her continual incitation for people to educate themselves on the subject of climate change has actually made a difference. This effect would be diametrically different from a chilling effect in that it pushes people to get educated on a subject, effectively arguably increasing their levels of freedom. This effect shall henceforth be called an \"Empowering effect\".\n",
    "\n",
    "First and foremost, the selected interest period depends on the choice of events that will be the pivots of the interrupted time series analysis. (ITS) These events are as follows: the school strike of August 20, 2018 led by Greta Thunberg, the 2018 United Nations Climate Change Conference (COP24) on December 14, 2018 and the summit of the United Nations on September 23, 2019. The period of analysis is between January 2018 and February 2020. As seen in HW1, the Covid-19 pandemic influences the pageviews a lots and we will therefore not elongate this analysis onto 2020.\n",
    "\n",
    "The data considered here comes from Wikipedia. We will consider 150 [ADAPT VALUE] Wikipedia articles divided into 3 groups. The first group is the treatment dataset, containing the Wikipedia articles related to climate change issues. The second group is a quasi-control group, where the topics considered are related to nature, without being related to climate change directly. The third group is another control group composed of popular articles simply reflecting the trends on Wikipedia. It is the same as the one used in the publication. The data sets considered give the number of pageviews per day for each article, within the time period considered.\n",
    "\n",
    "As already hinted at, the analysis will be very similar to the one used in the publication - an ITS analysis with segmented linear regressions.\n",
    "\n",
    "If the first group features significant changes in pageviews and the control groups don't across the selected period, we will be able to conclude that the \"Empowering Effect\" exists and we will be able to compare it to the chilling effect considered in the publication.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytical Methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform the comparison between the periods, we will use a trivial mean comparison as a non-model empirical findings in a first time.\n",
    "\n",
    "Then we will use a customized interrupted time serie analysis as depicted by Lagarde explained further. The purpose is to use a unique linear regression through all the periods with a dataset taking into account the influence of the data only on its corresponding period. \n",
    "\n",
    ">> Lagarde ITS : $$y_t = \\gamma_0 + \\gamma_1  \\times preslope + \\gamma_2  \\times intervention + \\gamma_3  \\times postslope + \\varepsilon_t$$\n",
    ">\n",
    ">This is the equation used by Lagarde (2012) mentioned in the paper where $y_t$ is the outcome at time $t$, $\\gamma_0$ is the baseline level at time 0, $\\gamma_1$ is the estimation of the trend in the outcome before the intervention, $\\gamma_2$ is the estimation of the structural structural trend without intervention and $\\gamma_3$ is the estimation of the trend in the outcome after the intervention. We can see how the data has to be structured to be used like this in the Table 2 from the said paper just below. The analyzed event from Table 2 happened in April 2006, hence we can see this month has the value 1 for \"intervention\" in the Table. It means **the month of the event has to be included in the second period for the regression computation**.\n",
    "\n",
    ">REFERENCES : Lagarde, Mylene. “How to Do (or Not to Do) … Assessing the Impact of a Policy Change with Routine Longitudinal Data.” Health Policy and Planning 27, no. 1 (January 1, 2012): 76–83. >https://doi.org/10.1093/heapol/czr004.\n",
    "> Table 2, page 80.\n",
    "\n",
    "><img src=\"./Lagarde.png\">\n",
    "\n",
    "We have modified the technique in order to be compliant with our data since we have multiple events contrary to Lagarde who computes the regression around one unique event. The modifications are as follow :\n",
    "\n",
    ">Each event $n$ has its proper column $intervention_n$ that is filled of 1 during the months correspond to the period after the event, where it has influence on. There are then $n+1$ columns correspond to each slope between the events, before the first event and after the last event. The equation for the linear regression is : $$y_t = \\gamma_0 + \\gamma_1  \\times intervention_1 + \\dotsm + \\gamma_n  \\times intervention_n + \\gamma_{n+1}  \\times slope_1 + \\dotsm + \\gamma_{2n+1}  \\times slope_{n+1} + \\varepsilon_t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having plotted the dataset, we will check outliers with the Cook's D distances on the linear regression and then we will detect the accountable articles with Z-score and its rule of Thumb ([-3:3]). \n",
    "\n",
    "We will check for auto-correlation/seasonality after this and correct it if necessary.\n",
    "\n",
    "Finally we will replot all the corrected dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data loading and data wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We simply load the csv's that have been extracted from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#~~~~~~~~~~~~~~~ Mainstream data ~~~~~~~~~~~~~~\n",
    "\n",
    "# Load the climate change articles data\n",
    "art_cc1 = pd.read_csv('data/pageviews-10-pro.csv').copy()\n",
    "art_cc2 = pd.read_csv('data/pageviews-20-pro.csv').copy()\n",
    "art_cc3 = pd.read_csv('data/pageviews-30-pro.csv').copy()\n",
    "art_cc4 = pd.read_csv('data/pageviews-40-pro.csv').copy()\n",
    "art_cc5 = pd.read_csv('data/pageviews-50-pro.csv').copy()\n",
    "\n",
    "# Load the control (popular) articles data\n",
    "art_control1 = pd.read_csv('data/pageviews-10-control.csv').copy()\n",
    "art_control2 = pd.read_csv('data/pageviews-20-control.csv').copy()\n",
    "art_control3 = pd.read_csv('data/pageviews-30-control.csv').copy()\n",
    "art_control4 = pd.read_csv('data/pageviews-40-control.csv').copy()\n",
    "# art_control5 = pd.read_csv('data/pageviews-50-control.csv').copy()\n",
    "\n",
    "\n",
    "#~~~~~~~~~~~~~~~ Scientific data ~~~~~~~~~~~~~~\n",
    "\n",
    "# Load the climate change articles science data\n",
    "sci_art_cc1 = pd.read_csv('data/Pro_climate/pageviews-group1-biodiv.csv').copy()\n",
    "sci_art_cc2 = pd.read_csv('data/Pro_climate/pageviews-group1-energy.csv').copy()\n",
    "sci_art_cc3 = pd.read_csv('data/Pro_climate/pageviews-group1-meteo.csv').copy()\n",
    "sci_art_cc4 = pd.read_csv('data/Pro_climate/pageviews-group1-pesticides.csv').copy()\n",
    "sci_art_cc5 = pd.read_csv('data/Pro_climate/pageviews-groupe1-earth.csv').copy()\n",
    "\n",
    "# Load the control articles science data\n",
    "sci_art_control1 = pd.read_csv('data/Group_control/pageviews-group2-anatomy.csv').copy()\n",
    "sci_art_control2 = pd.read_csv('data/Group_control/pageviews-group2-genetics.csv').copy()\n",
    "sci_art_control3 = pd.read_csv('data/Group_control/pageviews-group2-optic.csv').copy()\n",
    "sci_art_control4 = pd.read_csv('data/Group_control/pageviews-group2-philo.csv').copy()\n",
    "sci_art_control5 = pd.read_csv('data/Group_control/pageviews-group2-social.csv').copy()\n",
    "\n",
    "# Load the control articles science data\n",
    "sci_art_pop1 = pd.read_csv('data/Group_popular_2017/pageviews-control-2017.csv').copy()\n",
    "sci_art_pop2 = pd.read_csv('data/Group_popular_2017/pageviews-control-2017-1.csv').copy()\n",
    "sci_art_pop3 = pd.read_csv('data/Group_popular_2017/pageviews-control-2017-2.csv').copy()\n",
    "sci_art_pop4 = pd.read_csv('data/Group_popular_2017/pageviews-control-2017-3.csv').copy()\n",
    "sci_art_pop5 = pd.read_csv('data/Group_popular_2017/pageviews-control-2017-4.csv').copy()\n",
    "\n",
    "# Merge all datasets together for each corpus\n",
    "art_cc=art_cc1.merge(art_cc2).merge(art_cc3).merge(art_cc4).merge(art_cc5)\n",
    "art_pop=art_control1.merge(art_control2).merge(art_control3).merge(art_control4)#.merge(art_control5)\n",
    "sci_art_cc=sci_art_cc1.merge(sci_art_cc2).merge(sci_art_cc3).merge(sci_art_cc4).merge(sci_art_cc5)\n",
    "sci_art_control=sci_art_control1.merge(sci_art_control2).merge(sci_art_control3).merge(sci_art_control4).merge(sci_art_control5)\n",
    "sci_art_pop=sci_art_pop1.merge(sci_art_pop2).merge(sci_art_pop3).merge(sci_art_pop4).merge(sci_art_pop5)\n",
    "# Display to illustrate the structure\n",
    "sci_art_control.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Here we define all the intervals between the mentioned event about the climate : the school strike of August 20, 2018 led by Greta Thunberg, the 2018 United Nations Climate Change Conference (COP24) on December 14, 2018 and the summit of the United Nations on September 23, 2019. The modularity of the code allows to simply add or put out an event without changing the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INTERVALS_2 for the common data is shorter than INTERVALS\n",
    "INTERVALS_2 =[[\"2018-01\",\"2018-07\"],[\"2018-08\",\"2018-11\"],[\"2018-12\",\"2019-08\"],[\"2019-09\",\"2020-02\"]]\n",
    "#1er janvier 2017 jusquau 31 aout 2020\n",
    "INTERVALS =[(\"2017-01\",\"2018-07\"),(\"2018-08\",\"2018-11\"),(\"2018-12\",\"2019-08\"),(\"2019-09\",\"2020-08\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Here we have a function to aggregate the dataframe. It creates a dataframe with the index as the datetimes by month and with two columns as the number of the month and the pageviews count over all the article during the said month. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(df, freq=\"MS\", drop = None, events=INTERVALS, normalization=False) :\n",
    "    \"\"\"A function to aggregate the dataframe as a Serie sorted by month\n",
    "    with the total number of pageviews per month. With the numbering of each\n",
    "    month of the timeserie. drop is used to drop a list of articles in the \n",
    "    dataframe. freq is used to choice the frequency of the aggregation. \n",
    "    Add the columns intervention, preslope and postslope in the Lagarde fashion.\n",
    "    To perform the aggregation, it is necessary to give the date of the analyzed \n",
    "    event according to the chosen frequency. Apply a mean normalization according\n",
    "    to the boolean parameter.\"\"\"\n",
    "    df=df.copy()\n",
    "    df.index=df.Date\n",
    "    df.index=pd.to_datetime(df.index)\n",
    "#     df.index=df.index.ceil(freq) #round below according to a frequency\n",
    "    df=df.drop(columns=\"Date\")\n",
    "    #Recreate the dataframe without the named articles\n",
    "    if not drop == None :\n",
    "        df=df.drop(columns=drop)\n",
    "    \n",
    "    # At this point, we should  sum over all articles.\n",
    "    df=df.sum(axis=1)\n",
    "    # Agregate the data by months\n",
    "    df=df.groupby(pd.Grouper(freq=freq)).sum()\n",
    "    #Serie to dataframe\n",
    "    df=df.to_frame()\n",
    "    #Rename the column\n",
    "    df.columns=['pageviews']\n",
    "    #Add the column months to help to plot with the regression\n",
    "    months=np.arange(1, len(df.index)+1)\n",
    "    df[\"months\"]=months\n",
    "    #Create a column per event as intervention_i with \"1\" values during the timedelta of the event\n",
    "    for inters, nb in zip(events, range(0, len(events))) :\n",
    "        #Pass the first interval since we are not \"after\" a event\n",
    "        if inters == events[0] :\n",
    "            pass\n",
    "        else :\n",
    "            #Add the intervention column (0 before the event)\n",
    "            df[f\"intervention_{nb}\"]=[0 if (pd.Timestamp(inters[0])> month or month > pd.Timestamp(inters[1])) else 1 for month in df.index]\n",
    "    #Modular creation of the slope array. Consider a slope between two event, the values before\n",
    "    #the first event is 0 and are equal the month of the second event -1 after the second event as Lagarde's\n",
    "    for event_int, n_slope in zip(events, range(1, len(events)+1)) :\n",
    "        df[f\"slope_{n_slope}\"]=list(np.zeros(len(df), dtype=int))\n",
    "        idx_event0=df.at[event_int[0], \"months\"]\n",
    "        idx_event1=df.at[event_int[1], \"months\"]\n",
    "        #Fill with zero and start counting from the first event\n",
    "        df[f\"slope_{n_slope}\"]=df.apply(lambda x : int(x[\"months\"]-(idx_event0-1)) if x[\"months\"]>=idx_event0 else 0, axis=1)\n",
    "        #Mask all the values after the second event by the month of the event minus one\n",
    "        df[f\"slope_{n_slope}\"]=df.apply(lambda x : int(idx_event1-idx_event0+1) if x[\"months\"]>=idx_event1 else x[f\"slope_{n_slope}\"], axis=1)\n",
    "    #We just need to add one to the last value because we have defined the last entry of the datetime index of the dataframe as an event \n",
    "    #(e.g. the last value are duplicate [14, 14] and we want [14, 15])\n",
    "    df[f\"slope_{len(events)}\"].iat[-1]=df[f\"slope_{len(events)}\"].iat[-2]+1 \n",
    "    \n",
    "    # Apply normalization\n",
    "    if normalization :\n",
    "        norm_value = df[\"pageviews\"].mean()\n",
    "        df[\"pageviews\"]=df[\"pageviews\"].apply(lambda x: x/norm_value)\n",
    "\n",
    "    return df\n",
    "\n",
    "art_cc_agg=aggregate(df=art_cc,events=INTERVALS_2, normalization=True)\n",
    "art_pop_agg=aggregate(df=art_pop,events=INTERVALS_2, normalization=True)\n",
    "\n",
    "sci_art_cc_agg=aggregate(df=sci_art_cc, normalization=True)\n",
    "sci_art_control_agg=aggregate(df=sci_art_control, normalization=True)\n",
    "sci_art_pop_agg=aggregate(df=sci_art_pop, normalization=True)\n",
    "\n",
    "display(art_cc_agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2 Implementing the methods\n",
    "### 2.1 Computing the non model empirical comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">First, a very simple analysis is performed by doing the average monthly pageview count for each period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean view count per months for the different periods\n",
    "def periods_avg(df, intervals) :\n",
    "    monthly_mean = pd.DataFrame(data={'monthly_mean': [0] * len(INTERVALS)})\n",
    "    count=0\n",
    "    \n",
    "    for interval in intervals :\n",
    "        sub_df=df.loc[interval[0]:interval[1]]\n",
    "        mean=sub_df['pageviews'].mean()\n",
    "        monthly_mean.loc[count, 'monthly_mean'] = mean\n",
    "        count=count+1\n",
    "    \n",
    "    return monthly_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Common datasets\n",
    "\n",
    "art_cc_monthly_mean = periods_avg(art_cc_agg, INTERVALS_2)\n",
    "art_pop_monthly_mean = periods_avg(art_pop_agg, INTERVALS_2)\n",
    "\n",
    "#Scientific\n",
    "sci_art_cc_monthly_mean = periods_avg(sci_art_cc_agg, INTERVALS)\n",
    "sci_art_control_monthly_mean = periods_avg(sci_art_control_agg, INTERVALS)\n",
    "sci_art_pop_monthly_mean = periods_avg(sci_art_pop_agg, INTERVALS)\n",
    "\n",
    "sci_art_pop_monthly_mean\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Here we define a function to compute multiple linear regression according to intervals given as arguments. The function returns a dict with the summary of the results, the predicted values corresponding to the given pageviews/month dataframe and the upper/lower values of the confidence intervals for each regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2.2 Computing the Segmented Linear Regression parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Here we have a function to plot two dataframes, generally the studied group of Wikipedia articles and a selected comparator group. Each event is display with a vertical bar to separator each period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "import datetime\n",
    "from statsmodels.stats.outliers_influence import summary_table\n",
    "\n",
    "def get_intervals_from_lagarde(df) :\n",
    "    \"\"\"Allow to obtain the list of intervals between events as tuples of strings from the dataframe\n",
    "    in Lagarde fashion. \"\"\"\n",
    "    \n",
    "    #Obtain the intervals strings between the event.\n",
    "    #We want pairs of date with an offset of one between events so\n",
    "    #we create 2 lists starting with one event lag and zip the two ones\n",
    "    interval_early=[df.index[0].strftime('%Y-%m')]\\\n",
    "        +[df[df[column]==1].index[0].strftime('%Y-%m') for column in df.columns if column.startswith('intervention')] #[df[df[column]==1].index[0] = the first date beginning with 1 in the intervention column = the event\n",
    "    #Here it is complicated, we need to take the index of the first value \"1\" and substract\n",
    "    #one month to obtain the index correspond to the second element of the interval\n",
    "    interval_late=[(df[df[column]==1].index[0]-pd.DateOffset(months=1)).strftime('%Y-%m') for column in df.columns if column.startswith('intervention')]\\\n",
    "        +[df.index[-1].strftime('%Y-%m')] \n",
    "    return list(zip(interval_early, interval_late))\n",
    "    \n",
    "def lin_reg_period_lagarde(df, params={},CI=0.05) :\n",
    "    \"\"\"Function that returns the results of the OLS linear regression for the data points\n",
    "    around a particular event. The provided dataset must include one column 'intervention' \n",
    "    per event and a column per slope for each slope according to the Lagarde's format of\n",
    "    Interrupted Time Series. The value of each key\n",
    "    is a another dictionnary containing the results (summary), the array of predicted values\n",
    "    and the upper and lower values for the confidence interval. It is possible to specify the interval \n",
    "    of each period with before_interval and after_interval arguments as a list in params argument. \n",
    "    To give customized interval ->  params = {interval_number : ['2012-01','2013-05'], ...}\"\"\"\n",
    "\n",
    "    df=df.copy()\n",
    "    \n",
    "    #Get the list of intervals\n",
    "    intervals = get_intervals_from_lagarde(df)\n",
    "    #Check for customized interval\n",
    "    for interval_number in range(1,len(intervals)+1) :\n",
    "        intervals[interval_number-1]=params[interval_number] if interval_number in params.keys() else intervals[interval_number-1]   \n",
    "    print(intervals)\n",
    "    #Create the formula string (pageviews must be the first column)\n",
    "    formula=f\"{df.columns[0]} ~ \" + \" + \".join([column for column in df.columns[2:]])\n",
    "    #Linear regression with pageviews as dependent variable and other columns as indepedent variables as Lagarde.\n",
    "    #Make the regression only on the custom provided intervals \n",
    "    mod = smf.ols(formula=formula, data=df[intervals[0][0]:intervals[-1][1]])\n",
    "    res=mod.fit()\n",
    "    #Will store the data points separately for each period\n",
    "    results_dict={\"results\" : res}\n",
    "    #iterate through the intervals to compute their regression.\n",
    "    #Create the cumulative intercept value as depicted below\n",
    "    cumulative_intercept=res.params[0]\n",
    "    for interval, number_int in zip(intervals, list(range(1,len(intervals)+1,1))) :\n",
    "        #Extract the computed values (including upper and lower CI values)\n",
    "        _, summary_values, summary_names = summary_table(res, alpha=CI)\n",
    "        #Create a temporary dataframe with the result\n",
    "        df_res_tmp = pd.DataFrame(summary_values, columns=summary_names)\n",
    "#         display(df_res_tmp)  #-> To see how the data is structured\n",
    "        \n",
    "        #Extract the predicted values according to one period to plot them later \n",
    "        pages_predict=df_res_tmp.iloc[df[interval[0]][\"months\"][0]-1:df[interval[1]][\"months\"][0],:][\"Predicted\\nValue\"].T\n",
    "        #WARNING : It's a bit tricky but the column has always 95% in its name even if the CI\n",
    "        #introduce in alpha is different of 0.05. It has been tested and the values are different.\n",
    "        #Here we are taking only the values corresponding to the predictions of the current slope\n",
    "        predict_ci_low = df_res_tmp.iloc[df[interval[0]][\"months\"][0]-1:df[interval[1]][\"months\"][0],:][\"Mean ci\\n95% low\"].T\n",
    "        predict_ci_upp = df_res_tmp.iloc[df[interval[0]][\"months\"][0]-1:df[interval[1]][\"months\"][0],:][\"Mean ci\\n95% upp\"].T\n",
    "        \n",
    "        result={\n",
    "            \"predicts\" : pages_predict,\n",
    "            \"CI\" : {\n",
    "                \"lower\" : predict_ci_low,\n",
    "                \"upper\" : predict_ci_upp\n",
    "            }\n",
    "        }\n",
    "        results_dict[number_int]=result\n",
    "    return results_dict\n",
    "                                    \n",
    "\n",
    "        \n",
    "#       Create a dictionnary to pass the results. (See the structure below)\n",
    "\n",
    " \n",
    "    #Create a dictionnary to pass the results. The structure is as follow :\n",
    "#     dic = {\n",
    "#         \"result\" : res_before,\n",
    "#         \"interval1\" :{\n",
    "#             \"predicts\" : pages_predict_before,\n",
    "#             \"CI\" : {\n",
    "#                 \"lower\" : predict_ci_low_before,\n",
    "#                 \"upper\" : predict_ci_upp_before\n",
    "#             }\n",
    "#         }, \n",
    "#         \"interval2\" : {\n",
    "#             \"predicts\" : pages_predict_after,\n",
    "#             \"CI\" : {\n",
    "#                 \"lower\" : predict_ci_low_after,\n",
    "#                 \"upper\" : predict_ci_upp_after\n",
    "#             }\n",
    "#         },\n",
    "#         ...\n",
    "#     }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">We can try the function. The regressions are computed and will be used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "#Common dataset\n",
    "art_cc_reg=lin_reg_period_lagarde(df=art_cc_agg)\n",
    "art_pop_reg=lin_reg_period_lagarde(df=art_pop_agg)\n",
    "\n",
    "#Scienfitic dataset\n",
    "sci_art_control_reg=lin_reg_period_lagarde(df=sci_art_control_agg)\n",
    "sci_art_cc_reg=lin_reg_period_lagarde(df=sci_art_cc_agg);\n",
    "sci_art_pop_reg=lin_reg_period_lagarde(df=sci_art_pop_agg);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Displaying the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Non model empirical - Mean comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import (AutoMinorLocator, MultipleLocator)\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "def plot_means(dfs, params)  :\n",
    "    \"\"\"Function that plots the data to compare 2 ITS. It must be given the results of \n",
    "    the linear regression and the timeserie with the numbering by month. As the linear\n",
    "    regression function, it is possible to specify the interval of months for each period.\n",
    "    All the params are given through dictionnary. The dataframes are given as a tuple and the\n",
    "    corresponding results of the linear regression as well. The confidence area for each linear\n",
    "    regression is plotted as an area.\n",
    "    As follow : (df_studied, df_comparator)\"\"\"\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~ Extract parameter ~~~~~~~~~~~~~~~~~~~~~~\n",
    "    keys=params.keys()\n",
    "    title=params[\"title\"] if \"title\" in keys else \"\"\n",
    "    xlabel=params[\"xlabel\"] if \"xlabel\" in keys else \"\"\n",
    "    ylabel=params[\"ylabel\"] if \"ylabel\" in keys else \"\"\n",
    "    filename=params[\"filename\"] if \"filename\" in keys else \"no_name\"\n",
    "    #Names used to add in the legend\n",
    "    names=params[\"names\"] if \"names\" in keys else [\"\", \"\"]\n",
    "    intervals=params[\"intervals\"] if \"intervals\" in keys else []\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~ Create the empty plot ~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    fig, ax = plt.subplots(1,1,figsize= (7,7), sharey = True, sharex = True)\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~ Plotting all the data points of the serie ~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    #Plots the two plots, the second plot is the comparator group\n",
    "\n",
    "    ax.scatter(dfs[0].index, dfs[0], marker=\"o\", s=200, c='red', label=\"Climate change related articles\")\n",
    "    ax.scatter(dfs[1].index, dfs[1], marker=\"o\", s=200, c='black', label=\"Popular Articles\")\n",
    "\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~ Visual modifications of the plot ~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    ax.set_xlabel(xlabel, fontsize=12)\n",
    "    ax.set_ylabel(ylabel, fontsize=12)\n",
    "    ax.xaxis.grid(False)\n",
    "    #Plot tick bars\n",
    "    #Plot a vertical line to delimit the 2 intervals.\n",
    "    #-0.5 to locate the mid of the month. (Since the frequency is end of month)\n",
    "    \n",
    "    #Use to avoid \"duplicates\" in the legend\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    plt.legend(by_label.values(), by_label.keys(), loc='upper left',fontsize=12,bbox_to_anchor=(0.135, -0.15),ncol=2,frameon=True,edgecolor=\"black\")\n",
    "    fig.tight_layout()\n",
    "    fig.suptitle(title, y=1.07, fontsize=20)\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~ Save the plot in order to plot it further ~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    plt.savefig(f\"./plots/{filename}\", format=\"png\",bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.1.1 [COMMON] Climate change articles VS Popular group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "    \"title\" : \"[COMMON] Climate articles vs. Popular articles 2018, 2019\",\n",
    "    \"xlabel\" : \"Different periods\",\n",
    "    \"ylabel\" : \"Monthly mean of pageviews\",\n",
    "    \"filename\" : \"common-climate_vs_pop_MEAN.png\",\n",
    "    \"intervals\" : INTERVALS_2\n",
    "}\n",
    "dfs=(art_cc_monthly_mean, art_pop_monthly_mean)\n",
    "plot_means(dfs=dfs, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 [SCIENTIFIC] Climate change articles VS Quasi-control group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "    \"title\" : \"[SCIENTIFIC] Climate articles vs. Comparator articles (Quasi-control)\",\n",
    "    \"xlabel\" : \"Different periods\",\n",
    "    \"ylabel\" : \"Monthly mean of pageviews\",\n",
    "    \"filename\" : \"scientific-climate_vs_quasicontrol_MEAN.png\",\n",
    "    \"intervals\" : INTERVALS\n",
    "}\n",
    "dfs=(sci_art_cc_monthly_mean, sci_art_control_monthly_mean)\n",
    "plot_means(dfs=dfs, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 [SCIENTIFIC] Climate change articles VS Popular comparator group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "    \"title\" : \"[SCIENTIFIC] Climate articles vs. Popular articles 2017, 2018, 2019, 2020\",\n",
    "    \"xlabel\" : \"Different periods\",\n",
    "    \"ylabel\" : \"Monthly mean of pageviews\",\n",
    "    \"filename\" : \"scientific-climate_vs_pop_MEAN.png\",\n",
    "    \"intervals\" : INTERVALS\n",
    "}\n",
    "dfs=(sci_art_cc_monthly_mean, sci_art_pop_monthly_mean)\n",
    "plot_means(dfs=dfs, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3.2 Interrupted Time Serie (ITS) - Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Here we're defining the function to plot the ITS regression over months for 2 groups. The chosen events are displayed as vertical lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparator(dfs, lin_reg_res, params)  :\n",
    "    \"\"\"Function that plots the data to compare 2 ITS. It must be given the results of \n",
    "    the linear regression and the timeserie with the numbering by month. As the linear\n",
    "    regression function, it is possible to specify the interval of months for each period.\n",
    "    All the params are given through dictionnary. The dataframes are given as a tuple and the\n",
    "    corresponding results of the linear regression as well. The confidence area for each linear\n",
    "    regression is plotted as an area.\n",
    "    As follow : (df_studied, df_comparator)\"\"\"\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~ Extract parameter ~~~~~~~~~~~~~~~~~~~~~~\n",
    "    keys=params.keys()\n",
    "    title=params[\"title\"] if \"title\" in keys else \"\"\n",
    "    xlabel=params[\"xlabel\"] if \"xlabel\" in keys else \"\"\n",
    "    ylabel=params[\"ylabel\"] if \"ylabel\" in keys else \"\"\n",
    "    filename=params[\"filename\"] if \"filename\" in keys else \"no_name\"\n",
    "    #Names used to add in the legend\n",
    "    names=params[\"names\"] if \"names\" in keys else [\"\", \"\"]\n",
    "    ci=params[\"ci\"] if \"ci\" in keys else \"\"\n",
    "    #axis limits\n",
    "    TOP=params[\"top\"] if \"top\" in keys else None\n",
    "    BOTTOM=params[\"bottom\"] if \"bottom\" in keys else None\n",
    "    \n",
    "        \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~ Custom intervals ~~~~~~~~~~~~~~~~~~~~~~\n",
    "    #Get the list of intervals\n",
    "    intervals = get_intervals_from_lagarde(dfs[0])\n",
    "    \n",
    "    #Check for customized interval\n",
    "    for interval_number in range(1,len(intervals)+1) :\n",
    "        if \"custom_intervals\" in keys :\n",
    "            intervals[interval_number-1]=params[\"custom_intervals\"][interval_number] if interval_number in params[\"custom_intervals\"].keys() else intervals[interval_number-1]\n",
    "    \n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~ Fix limit of the axes ~~~~~~~~~~~~~~~~~~~~~~ \n",
    "    \n",
    "    #Boundaries of the y-axis, it has been decided to keep them constant instead of relative by % of the\n",
    "    #max and min value because it is easier to compare plot when the axes are constant\n",
    "    BOTTOM=0 if BOTTOM is None else BOTTOM\n",
    "    all_pageviews=[j for i in [list(dfs[0][\"pageviews\"]), list(dfs[1][\"pageviews\"])] for j in i]\n",
    "    MAX=max(all_pageviews)\n",
    "    TOP=MAX+0.1*MAX if TOP is None else TOP#1500000#100000000 \n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~ Create the empty plot ~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    fig, ax = plt.subplots(1,1,figsize= (11,7), sharey = True, sharex = True)\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~ Plotting all the data points of the serie ~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    #Choose in advance the colors of the two groups\n",
    "#     colors=[(\"black\", \"dimgray\"), (\"darkgrey\", \"silver\")]\n",
    "    colors=[(\"darkred\", \"firebrick\", \"indianred\"), (\"mediumaquamarine\", \"aquamarine\", \"paleturquoise\")]\n",
    "    #Plots the two plots, the second plot is the comparator group\n",
    "    for df, reg, col, name in zip(dfs, lin_reg_res, colors, names) :\n",
    "        #Just to have the correct keys for the reg dict and select\n",
    "        #properly the right period\n",
    "        name_interval=[\"-\".join(inter) for inter in intervals]\n",
    "        \n",
    "        ax.scatter(x=df[\"months\"],y=df[\"pageviews\"], marker=\"o\", s=50, c=col[0], label=f\"{name}-related Articles\")\n",
    "        #Plotting linear regression\n",
    "        for interval, n_inter, key in zip(intervals, name_interval,  list(range(1,len(intervals)+1,1))) :\n",
    "            ax.plot(df.loc[interval[0]:interval[1]][\"months\"],\n",
    "                    reg[key][\"predicts\"], #Take the data points of the regression from the dict\n",
    "                    linewidth=4,\n",
    "                    alpha=0.9,\n",
    "                    c=col[1],\n",
    "                    label=f\"{name} Article Trend\") #{n_inter}\n",
    "        \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~ CI plot ~~~~~~~~~~~~~~~~~~~~~~\n",
    "        #For each regression line, plot the upper CI values as a line and\n",
    "        #the lower CI values. Filled the area between the two.\n",
    "        periods = list(reg.values())[1:]\n",
    "        for period, interval in zip(periods, intervals) :\n",
    "            for _, ci_array in period[\"CI\"].items() :\n",
    "                ax.plot(df.loc[interval[0]:interval[1]][\"months\"],\n",
    "                        ci_array,\n",
    "                        linewidth=2,\n",
    "                        alpha=0.1,\n",
    "                        c=col[2]\n",
    "                       )\n",
    "            #Create a color area representing the CI\n",
    "            plt.fill_between(df.loc[interval[0]:interval[1]][\"months\"],\n",
    "                period[\"CI\"][\"lower\"],\n",
    "                period[\"CI\"][\"upper\"],\n",
    "                where=period[\"CI\"][\"upper\"] >= period[\"CI\"][\"lower\"],\n",
    "                facecolor=col[2], alpha=0.25, interpolate=True, label=f\"{name} Confidence Interval ({ci})\")\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~ Visual modifications of the plot ~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    ax.set_ylim(bottom=BOTTOM, top=TOP)\n",
    "    ax.set_xlabel(xlabel, fontsize=12)\n",
    "    ax.set_ylabel(ylabel, fontsize=12)\n",
    "    ax.xaxis.grid(False)\n",
    "    ax.xaxis.set_major_locator(MultipleLocator(2))\n",
    "    ax.xaxis.set_minor_locator(AutoMinorLocator(4))\n",
    "    ax.xaxis.set_tick_params(length = 5, width = 1)\n",
    "    ax.yaxis.set_tick_params(length = 5, width = 1)\n",
    "    #Plot tick bars\n",
    "    ax.ticklabel_format(useOffset=False, style='plain')\n",
    "    #Plot a vertical line to delimit the 2 intervals.\n",
    "    #-0.5 to locate the mid of the month. (Since the frequency is end of month)\n",
    "    for interval, counter in zip(intervals, list(range(1,len(intervals)+1,1)))  :\n",
    "        #The event is located always as the first month of an interval except the first one\n",
    "        if counter > 1 :\n",
    "            event=float(df.loc[interval[0]][\"months\"]-0.5)\n",
    "            plt.axvline(x=event, color = 'black', alpha=1, linewidth=3.5)\n",
    "            #Indicate the mid June (Arbitrary addition values to center the label)\n",
    "            ax.text(event-1.1, TOP, interval[0], fontsize=13)\n",
    "    #Use to avoid \"duplicates\" in the legend\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    plt.legend(by_label.values(), by_label.keys(), loc='upper left',fontsize=12,bbox_to_anchor=(0.135, -0.15),ncol=2,frameon=True,edgecolor=\"black\")\n",
    "    fig.tight_layout()\n",
    "    fig.suptitle(title, y=1.07, fontsize=20)\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~ Save the plot in order to plot it further ~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    plt.savefig(f\"./plots/{filename}\", format=\"png\",bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 [COMMON] Climate change articles VS Popular articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the function \n",
    "params={\n",
    "    \"title\" : \"[COMMON] Climate articles vs. Popular articles 2018, 2019\",\n",
    "    \"xlabel\" : \"Time (Months)\",\n",
    "    \"ylabel\" : \"Total Views (All Articles)\",\n",
    "    \"filename\" : \"common-climate_vs_pop.png\",\n",
    "    \"ci\" : \"95%\",\n",
    "    \"names\" : (\"Climate\",\"Popular\"),\n",
    "    \"intervals\" : INTERVALS_2,\n",
    "#     \"bottom\" : 0.6,\n",
    "#     \"top\" : 1.4\n",
    "}\n",
    "dfs=(art_cc_agg,art_pop_agg)\n",
    "regs=(art_cc_reg, art_pop_reg)\n",
    "plot_comparator(dfs=dfs, lin_reg_res=regs, params=params)\n",
    "\n",
    "print(f\"[COMMON] Climate change | Linear Regression interval 1 :\\n\\n {art_cc_reg['results'].summary()}\\n\\n\")\n",
    "print(f\"[COMMON] Popular articles | Linear Regression interval 1 :\\n\\n {art_pop_reg['results'].summary()}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 [SCIENTIFIC] Climate change articles VS Quasi-control group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the function \n",
    "params={\n",
    "    \"title\" : \"[SCIENTIFIC] Climate articles vs. Comparator articles (Quasi-control)\",\n",
    "    \"xlabel\" : \"Time (Months)\",\n",
    "    \"ylabel\" : \"Total Views (All Articles)\",\n",
    "    \"filename\" : \"scientific-climate_vs_quasicontrol.png\",\n",
    "    \"ci\" : \"95%\",\n",
    "    \"names\" : (\"Climate\",\"Quasi-control\"),\n",
    "    \"intervals\" : INTERVALS,\n",
    "#     \"bottom\" : 0.6,\n",
    "#     \"top\" : 1.4\n",
    "}\n",
    "dfs=(sci_art_cc_agg, sci_art_control_agg)\n",
    "regs=(sci_art_cc_reg, sci_art_control_reg)\n",
    "plot_comparator(dfs=dfs, lin_reg_res=regs, params=params)\n",
    "\n",
    "print(f\"[SCIENTIFIC] Climate change | Linear Regression interval 1 :\\n\\n {sci_art_cc_reg['results'].summary()}\\n\\n\")\n",
    "print(f\"[SCIENTIFIC] Quasi-control | Linear Regression interval 1 :\\n\\n {sci_art_control_reg['results'].summary()}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">In the publication, the terrorism articles are compared to another quasi-control group. This group is made or security related articles that would arguably not be affected by the treatment but are similar enough in terms of topic to be used as a control group. The following cells present an attempt at the replication of that method.\n",
    "\n",
    "> To be able to reproduce this method in our context, we need a group of articles that are likely to be affected by the treatment, namely that people would consult them more because of the mediatic movement around Greta Thunberg and climate change. The second group should be a group of articles that are similar to the first group such that any external bias would appear in both and therefore cancel out at comparison. It should also not be affected by the treatment, namely not be related to climate change issues. It is not straight forward to find a second group that would be used along with the first corpus that we considered, the corpus of the climate change related articles plotted above because this group is already quite broad. \n",
    "\n",
    "> To paliate to this problem, we restrained ourselves to scientific articles chosen in wikipedia index of articles randomly. The first group was selected randomly in a list of scientific articles on topics likely to be related to climate change (biodiversity, energy, meteo, pesticides, earth). The second group was selected randomly in a list of scientific articles on topics not related to climate change (anatomy, genetics, optics, philosophy, social). This pick has the advantage of showing if the public reads on scientific articles, which would confirm the hypothesis that people were encouraged to get educated on the topic of climate change. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3.2.3 [SCIENTIFIC] Climate change articles VS Popular comparator group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the function \n",
    "params={\n",
    "    \"title\" : \"[SCIENTIFIC] Climate articles vs. Popular articles 2017, 2018, 2019, 2020\",\n",
    "    \"xlabel\" : \"Time (Months)\",\n",
    "    \"ylabel\" : \"Total Views (All Articles)\",\n",
    "    \"filename\" : \"scientific-climate_vs_pop.png\",\n",
    "    \"ci\" : \"95%\",\n",
    "    \"names\" : (\"Climate\",\"Popular\"),\n",
    "    \"intervals\" : INTERVALS,\n",
    "#     \"bottom\" : 0.4,\n",
    "#     \"top\" : 1.8\n",
    "}\n",
    "dfs=(sci_art_cc_agg, sci_art_pop_agg)\n",
    "regs=(sci_art_cc_reg, sci_art_pop_reg)\n",
    "plot_comparator(dfs=dfs, lin_reg_res=regs, params=params)\n",
    "\n",
    "print(f\"[SCIENTIFIC] Climate change | Linear Regression interval 1 :\\n\\n {sci_art_cc_reg['results'].summary()}\\n\\n\")\n",
    "print(f\"[SCIENTIFIC] Popular articles | Linear Regression interval 1 :\\n\\n {sci_art_pop_reg['results'].summary()}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4 Checking correctness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Checking outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly we can check the Cooks D values from the regression of each dataset to detect the presence of potential outliers.\n",
    "\n",
    ">Details : Cook's D \n",
    "\n",
    ">>><img src=\"./cooksD.png\">\n",
    "\n",
    ">> Where $y_j$ — the jth fitted response value, $y_{j(i)}$ — the jth fitted response value, where the fit does not include observation $i$, $p$ — the number of regression coefficients, $\\sigma$ — the estimated variance from the fit, based on all observations, i.e. Mean Squared Error\n",
    "\n",
    ">>If a data point has a Cook’s distance of more than three times the mean, it is a possible outlier. Any point over 4/n, where n is the number of observations, should be examined. (https://medium.com/@lymielynn/a-little-closer-to-cooks-distance-e8cc923a3250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.stats.outliers_influence\n",
    "#Give the list of Cook's D distance from the result of the regression\n",
    "for name, reg in zip((\"Climate change [COMMON]\",\"Popular articles [COMMON]\",\"Climate change [SCIENTIFIC]\", \"Quasi-control [SCIENTIFIC]\", \"Popular articles [SCIENTIFIC]\"),(art_cc_reg, art_pop_reg,sci_art_cc_reg, sci_art_control_reg, sci_art_pop_reg)) :\n",
    "    plt.figure()\n",
    "    cooksD=statsmodels.stats.outliers_influence.OLSInfluence(reg[\"results\"]).cooks_distance\n",
    "    print(f\"{name} :\\n{cooksD[0].sort_values(ascending=False).head(10)}\\n\")\n",
    "    print(f\"Threshold for outlier according to the mean \\t: {cooksD[0].mean()*3}\")\n",
    "    print(f\"Threshold for outlier according to 4/n \\t\\t: {4/len(cooksD[0])}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there are some outlieres in each dataset. Let's use the z-score to find the article that creates this effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify the presence of outliers using the z-score as the paper did. Whether an article has a z-score superior to 3.0 or inferior to -3.0, it might be considered as an outlier. The z-score will be computed of the aggregated dataframe grouped by article.\n",
    "\n",
    ">Z-score\n",
    ">><img src=\"./zscore.png\">\n",
    "\n",
    ">>Where $x_i$ is an entry, $\\overline{x}$ the mean of the data and $s$ the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore(df) :\n",
    "    \"Compute the z-score for all articles\"\n",
    "    df=df.copy().T.sum(axis=1).drop(index=\"Date\")\n",
    "    df=df.apply(lambda x : (x-df.mean())/df.std())\n",
    "    \n",
    "    return df\n",
    "\n",
    "for name, df in zip((\"Climate change [COMMON]\",\"Popular articles [COMMON]\",\"Climate change [SCIENTIFIC]\", \"Quasi-control [SCIENTIFIC]\", \"Popular articles [SCIENTIFIC]\"),(art_cc, art_pop,sci_art_cc, sci_art_control, sci_art_pop)) :\n",
    "    plt.figure()\n",
    "    \n",
    "    print(f\"{name} :\\n{zscore(df).sort_values(ascending=False).head(10)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[COMMON] Global warming is apparently a large outlier, unfortunately, this is the main article related to our problmatic, it has been decided to keep it in the dataset since it is very relevant.\n",
    "\n",
    "[SCIENTIFIC] We can consider that the outliers for the climate change dataset are Health, Life expectancy and Humanism, it is logical since these themes are linked to other matter that only the climate change. For the control dataset about scientific articles, we can see the Galileo Galilei and Autism are outliers. Finally Donald Trump is an outlier for the popular articles. We can drop them in the aggregate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "art_cc_agg_wo=aggregate(df=art_cc, drop=[], normalization=True, events=INTERVALS_2)\n",
    "art_pop_agg_wo=aggregate(df=art_pop, drop=[], normalization=True, events=INTERVALS_2)\n",
    "\n",
    "sci_art_cc_agg_wo=aggregate(df=sci_art_cc, drop=[\"Health\", \"Life expectancy\"], normalization=True)\n",
    "sci_art_control_agg_wo=aggregate(df=sci_art_control, drop=[\"Galileo Galilei\", \"Autism\"], normalization=True)\n",
    "sci_art_pop_agg_wo=aggregate(df=sci_art_pop, drop=[\"Donald Trump\"], normalization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Checking auto-correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear regression of a time serie may suffer of auto-correlation : this is the fact that a given value might be influenced by a previous value of the time serie according a certain lag. (e.g. a year seasonality with a lag of 365).\n",
    "The authors have used the test of Prais-Winsten to detect the auto-correlation. Here we used the plot_acf() function from statsmodel to detect the presence of autocorrelation according to all the possible values of lag. Whether the auto-correlation value is not in the confidence interval, the lag is considered to have an auto-correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "\n",
    "for name, df in zip((\"Climate change [COMMON]\",\"Popular articles [COMMON]\",\"Climate change [SCIENTIFIC]\", \"Quasi-control [SCIENTIFIC]\", \"Popular articles [SCIENTIFIC]\"),(art_cc_agg_wo, art_pop_agg_wo,sci_art_cc_agg_wo, sci_art_control_agg_wo, sci_art_pop_agg_wo)) :\n",
    "    plt.figure()\n",
    "    ax1=plot_acf(df['pageviews'], lags=int(len(df['pageviews'])-1), title=f\"Autocorrelation of {name}\");\n",
    "    plt.xlabel(\"Lags (months)\", fontsize=12)\n",
    "    plt.ylabel(\"Autocorrelation (0=None)\", fontsize=12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there are correlations of lag 1 and 2 along all the datasets. Except for popular articles, it is logical since the popular articles are trendy during a short period then their count of pageviews doesn't evolve much for the remaining period.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statistics\n",
    "# def correct_autocorrelation(df, lag):\n",
    "#     \"\"\"Compute all the differences between 2 elements distant by a certain shift suffering \n",
    "#     from autocorrelation. Then the difference is added to the original value of the timeserie\n",
    "#     to correct the autocorrelation. The drawback is that the first value of the timeserie cannot\n",
    "#     be correct since we have not the previous value according to the lag.\"\"\"\n",
    "#     diff = list()\n",
    "#     df=df.copy()\n",
    "#     for i in range(lag, len(df)):\n",
    "#         value = df[\"pageviews\"].iat[i] - df[\"pageviews\"].iat[i - lag]\n",
    "#         diff.append(value)\n",
    "#     #Correct all the values according to the lag except the first one that cannot be corrected without the previous value\n",
    "#     #Mean imputation\n",
    "#     diff=[statistics.mean(diff)]+diff\n",
    "#     df[\"pageviews\"]=diff\n",
    "# #     df[\"pageviews\"].iloc[1:]=df[\"pageviews\"].iloc[1:]+diff\n",
    "#     return df\n",
    "\n",
    "\n",
    "# domest_ag_wo_wa=correct_autocorrelation(domest_ag_wo, 1)\n",
    "# plt.figure()\n",
    "# ax1=plot_acf(domest_ag_wo_wa['pageviews'], lags=int(len(domest_ag_wo_wa['pageviews'])-1), title=\"Autocorrelation of corrected domestic-related dataset over months\");\n",
    "# plt.xlabel(\"Lags (months)\", fontsize=12)\n",
    "# plt.ylabel(\"Autocorrelation (0=None)\", fontsize=12)\n",
    "\n",
    "# ter30_ag_wa=correct_autocorrelation(ter30_ag, 1)\n",
    "# plt.figure()\n",
    "# ax1=plot_acf(ter30_ag_wa['pageviews'], lags=int(len(ter30_ag_wa['pageviews'])-1), title=\"Autocorrelation of corrected terrorism-related dataset over months\");\n",
    "# plt.xlabel(\"Lags (months)\", fontsize=12)\n",
    "# plt.ylabel(\"Autocorrelation (0=None)\", fontsize=12)\n",
    "\n",
    "# We reduced the autocorrelation of lag 1 to a level very close to the confidence interval. The result is deemed acceptable to continue. It should be noted, however, that the authors did not detect autocorrelation at this stage but at the following one, which marks a difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Replotting the corrected datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recompute linear regression\n",
    "art_pop_reg_wo=lin_reg_period_lagarde(df=art_pop_agg_wo)\n",
    "art_cc_reg_wo=lin_reg_period_lagarde(df=art_cc_agg_wo);\n",
    "\n",
    "sci_art_control_reg_wo=lin_reg_period_lagarde(df=sci_art_control_agg_wo)\n",
    "sci_art_cc_reg_wo=lin_reg_period_lagarde(df=sci_art_cc_agg_wo);\n",
    "sci_art_pop_reg_wo=lin_reg_period_lagarde(df=sci_art_pop_agg_wo);\n",
    "\n",
    "#Recompute mean\n",
    "art_cc_monthly_mean_wo = periods_avg(art_cc_agg_wo, INTERVALS_2)\n",
    "art_pop_monthly_mean_wo = periods_avg(art_pop_agg_wo, INTERVALS_2)\n",
    "\n",
    "sci_art_cc_monthly_mean_wo = periods_avg(sci_art_cc_agg_wo, INTERVALS)\n",
    "sci_art_control_monthly_mean_wo = periods_avg(sci_art_control_agg_wo, INTERVALS)\n",
    "sci_art_pop_monthly_mean_wo = periods_avg(sci_art_pop_agg_wo, INTERVALS)\n",
    "\n",
    "\n",
    "#Plot mean comparison\n",
    "params={\n",
    "    \"title\" : \"[COMMON] Climate articles vs. Popular articles 2018, 2019\\nWithout outliers\",\n",
    "    \"xlabel\" : \"Different periods\",\n",
    "    \"ylabel\" : \"Monthly mean of pageviews\",\n",
    "    \"filename\" : \"common-climate_vs_pop_MEAN_wo.png\",\n",
    "    \"intervals\" : INTERVALS_2\n",
    "}\n",
    "dfs=(art_cc_monthly_mean_wo, art_pop_monthly_mean_wo)\n",
    "plot_means(dfs=dfs, params=params)\n",
    "\n",
    "\n",
    "params={\n",
    "    \"title\" : \"[SCIENTIFIC] Climate articles vs. Comparator articles (Quasi-control)\\nWithout outliers\",\n",
    "    \"xlabel\" : \"Different periods\",\n",
    "    \"ylabel\" : \"Monthly mean of pageviews\",\n",
    "    \"filename\" : \"scientific-climate_vs_quasicontrol_MEAN_wo.png\",\n",
    "    \"intervals\" : INTERVALS\n",
    "}\n",
    "dfs=(sci_art_cc_monthly_mean_wo, sci_art_control_monthly_mean_wo)\n",
    "plot_means(dfs=dfs, params=params)\n",
    "\n",
    "params={\n",
    "    \"title\" : \"[SCIENTIFIC] Climate articles vs. Popular articles 2017, 2018, 2019, 2020\\nWithout outliers\",\n",
    "    \"xlabel\" : \"Different periods\",\n",
    "    \"ylabel\" : \"Monthly mean of pageviews\",\n",
    "    \"filename\" : \"scientific-climate_vs_pop_MEAN_wo.png\",\n",
    "    \"intervals\" : INTERVALS\n",
    "}\n",
    "dfs=(sci_art_cc_monthly_mean_wo, sci_art_pop_monthly_mean_wo)\n",
    "plot_means(dfs=dfs, params=params)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Plot regression comparison \n",
    "\n",
    "params={\n",
    "    \"title\" : \"[COMMON] Climate articles vs. Popular articles 2018, 2019\\nWithout outliers\",\n",
    "    \"xlabel\" : \"Time (Months)\",\n",
    "    \"ylabel\" : \"Total Views (All Articles)\",\n",
    "    \"filename\" : \"common-climate_vs_pop_wo.png\",\n",
    "    \"ci\" : \"95%\",\n",
    "    \"names\" : (\"Climate\",\"Popular\"),\n",
    "    \"intervals\" : INTERVALS,\n",
    "#     \"bottom\" : 0,\n",
    "#     \"top\" : 1.1\n",
    "}\n",
    "dfs=(art_cc_agg_wo, art_pop_agg_wo)\n",
    "regs=(art_cc_reg_wo, art_pop_reg_wo)\n",
    "plot_comparator(dfs=dfs, lin_reg_res=regs, params=params)\n",
    "\n",
    "print(f\"Climate change | Linear Regression interval 1 :\\n\\n {sci_art_cc_reg['results'].summary()}\\n\\n\")\n",
    "print(f\"Popular articles | Linear Regression interval 1 :\\n\\n {sci_art_pop_reg['results'].summary()}\\n\\n\")\n",
    "\n",
    "params={\n",
    "    \"title\" : \"[SCIENTIFIC] Climate articles vs. Comparator articles (Quasi-control)\\nWithout outliers\",\n",
    "    \"xlabel\" : \"Time (Months)\",\n",
    "    \"ylabel\" : \"Total Views (All Articles)\",\n",
    "    \"filename\" : \"scientific-climate_vs_quasicontrol_wo.png\",\n",
    "    \"ci\" : \"95%\",\n",
    "    \"names\" : (\"Climate\",\"Quasi-control\"),\n",
    "    \"intervals\" : INTERVALS,\n",
    "#     \"bottom\" : 0,\n",
    "#     \"top\" : 1.1\n",
    "}\n",
    "dfs=(sci_art_cc_agg_wo, sci_art_control_agg_wo)\n",
    "regs=(sci_art_cc_reg_wo, sci_art_control_reg_wo)\n",
    "plot_comparator(dfs=dfs, lin_reg_res=regs, params=params)\n",
    "\n",
    "print(f\"[SCIENTIFIC] Climate change | Linear Regression interval 1 :\\n\\n {sci_art_cc_reg['results'].summary()}\\n\\n\")\n",
    "print(f\"[SCIENTIFIC] Quasi-control | Linear Regression interval 1 :\\n\\n {sci_art_control_reg['results'].summary()}\\n\\n\")\n",
    "\n",
    "params={\n",
    "    \"title\" : \"[SCIENTIFIC] Climate articles vs. Popular articles 2017, 2018, 2019, 2020\\nWithout outliers\",\n",
    "    \"xlabel\" : \"Time (Months)\",\n",
    "    \"ylabel\" : \"Total Views (All Articles)\",\n",
    "    \"filename\" : \"scientific-climate_vs_pop_wo.png\",\n",
    "    \"ci\" : \"95%\",\n",
    "    \"names\" : (\"Climate\",\"Quasi-control\"),\n",
    "    \"intervals\" : INTERVALS,\n",
    "#     \"bottom\" : 0,\n",
    "#     \"top\" : 1.1\n",
    "}\n",
    "dfs=(sci_art_cc_agg_wo, sci_art_pop_agg_wo)\n",
    "regs=(sci_art_cc_reg_wo, sci_art_pop_reg_wo)\n",
    "plot_comparator(dfs=dfs, lin_reg_res=regs, params=params)\n",
    "\n",
    "print(f\"[SCIENTIFIC] Climate change | Linear Regression interval 1 :\\n\\n {sci_art_cc_reg['results'].summary()}\\n\\n\")\n",
    "print(f\"[SCIENTIFIC] Popular articles | Linear Regression interval 1 :\\n\\n {sci_art_pop_reg['results'].summary()}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ada]",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
