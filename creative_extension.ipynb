{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project milestone 4\n",
    "Tadaa - Jonathan Haenni, Lea Schmidt, Danny Kohler\n",
    "> This work presents the creative extension of the publication from J. Penney : \"Chilling effects : Online surveillance and Wikipedia use\".\n",
    "\n",
    "---\n",
    "\n",
    "## Abstract\n",
    "The goal of this work is to observe if the public action of Greta Thunberg and her continual incitation for people to educate themselves on the subject of climate change has actually made a difference. This effect would be diametrically different from a chilling effect in that it pushes people to get educated on a subject, effectively arguably increasing their levels of freedom. This effect shall henceforth be called an \"Empowering effect\".\n",
    "\n",
    "First and foremost, the selected interest period depends on the choice of events that will be the pivots of the interrupted time series analysis. (ITS) These events are as follows: the school strike of August 20, 2018 led by Greta Thunberg, the 2018 United Nations Climate Change Conference (COP24) on December 14, 2018 and the summit of the United Nations on September 23, 2019. The period of analysis is between January 2018 and February 2020. As seen in HW1, the Covid-19 pandemic influences the pageviews a lots and we will therefore not elongate this analysis onto 2020.\n",
    "\n",
    "The data considered here comes from Wikipedia. We will consider 150 [ADAPT VALUE] Wikipedia articles divided into 3 groups. The first group is the treatment dataset, containing the Wikipedia articles related to climate change issues. The second group is a quasi-control group, where the topics considered are related to nature, without being related to climate change directly. The third group is another control group composed of popular articles simply reflecting the trends on Wikipedia. It is the same as the one used in the publication. The data sets considered give the number of pageviews per day for each article, within the time period considered.\n",
    "\n",
    "As already hinted at, the analysis will be very similar to the one used in the publication - an ITS analysis with segmented linear regressions.\n",
    "\n",
    "If the first group features significant changes in pageviews and the control groups don't across the selected period, we will be able to conclude that the \"Empowering Effect\" exists and we will be able to compare it to the chilling effect considered in the publication.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Data loading and filtering\n",
    "\n",
    "> code description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the climate change articles data\n",
    "art_cc1 = pd.read_csv('data/pageviews-10-pro.csv').copy()\n",
    "art_cc2 = pd.read_csv('data/pageviews-20-pro.csv').copy()\n",
    "art_cc3 = pd.read_csv('data/pageviews-30-pro.csv').copy()\n",
    "art_cc4 = pd.read_csv('data/pageviews-40-pro.csv').copy()\n",
    "art_cc5 = pd.read_csv('data/pageviews-50-pro.csv').copy()\n",
    "\n",
    "# Load the control (popular) articles data\n",
    "art_control1 = pd.read_csv('data/pageviews-10-control.csv').copy()\n",
    "art_control2 = pd.read_csv('data/pageviews-20-control.csv').copy()\n",
    "art_control3 = pd.read_csv('data/pageviews-30-control.csv').copy()\n",
    "art_control4 = pd.read_csv('data/pageviews-40-control.csv').copy()\n",
    "#art_control5 = pd.read_csv('data/pageviews-50-control.csv').copy()\n",
    "\n",
    "# Merge all datasets together\n",
    "art_cc=art_cc1.merge(art_cc2).merge(art_cc3).merge(art_cc4).merge(art_cc5)\n",
    "art_control=art_control1.merge(art_control2).merge(art_control3).merge(art_control4)#.merge(art_control5)\n",
    "\n",
    "# Display to illustrate the structure\n",
    "art_control.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Translate dates in datetime type\n",
    "# art_cc.Date=pd.to_datetime(art_cc.Date)\n",
    "# art_control.Date=pd.to_datetime(art_control.Date)\n",
    "\n",
    "# # Set the date as the index\n",
    "# art_cc=art_cc.set_index('Date')\n",
    "# art_control=art_control.set_index('Date')\n",
    "\n",
    "# # Agregate the data by months\n",
    "# art_cc=art_cc.groupby(pd.Grouper(freq=\"M\")).sum()\n",
    "# art_control=art_control.groupby(pd.Grouper(freq=\"M\")).sum()\n",
    "\n",
    "# # Display to illustrate the structure\n",
    "# display(art_control.head())\n",
    "\n",
    "# # At this point, we should  sum over all articles.\n",
    "# art_cc=art_cc.sum(axis=1)\n",
    "# art_control=art_control.sum(axis=1)\n",
    "\n",
    "# # Here the index is reset to get the number of months instead of dates, as in the publication.\n",
    "# # The date column is then removed : it is not needed anymore\n",
    "# # The pageviews column is renamed\n",
    "# art_cc=art_cc.reset_index()\n",
    "# art_cc=art_cc.drop('Date', axis=1)\n",
    "# art_cc.columns=['pageviews']\n",
    "\n",
    "# art_control=art_control.reset_index()\n",
    "# art_control=art_control.drop('Date', axis=1)\n",
    "# art_control.columns=['pageviews']\n",
    "\n",
    "# # Display to illustrate the structure\n",
    "# display(art_control.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Here we have a function to aggregate the dataframe. It creates a dataframe with the index as the datetimes by month and with two columns as the number of the month and the pageviews count over all the article during the said month. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Je propose cette fonction pour aggrÃ©ger directement mais en pouvant choisir quel article drop \n",
    "#s'il y a des outliers. En plus les dates sont en index ce qui permet d'utiliser les features de Pandas\n",
    "\n",
    "def aggregate(df, freq=\"M\", drop = None) :\n",
    "    \"\"\"A function to aggregate the dataframe as a Serie sorted by month\n",
    "    with the total number of pageviews per month. With the numbering of each\n",
    "    month of the timeserie. drop is used to drop a list of articles in the \n",
    "    dataframe. freq is used to choice the frequency of the aggregation. Return the dataframe\"\"\"\n",
    "    df=df.copy()\n",
    "    df.index=df.Date\n",
    "    df.index=pd.to_datetime(df.index)\n",
    "    df=df.drop(columns=\"Date\")\n",
    "    #Recreate the dataframe without the named articles\n",
    "    if not drop == None :\n",
    "        df=df.drop(columns=drop)\n",
    "    \n",
    "    # At this point, we should  sum over all articles.\n",
    "    df=df.sum(axis=1)\n",
    "    # Agregate the data by months\n",
    "    df=df.groupby(pd.Grouper(freq=freq)).sum()\n",
    "    #Serie to dataframe\n",
    "    df=df.to_frame()\n",
    "    #Rename the column\n",
    "    df.columns=['pageviews']\n",
    "    #Add the column months to help to plot with the regression\n",
    "    months=np.arange(1, len(df.index)+1)\n",
    "    df[\"months\"]=months\n",
    "    return df\n",
    "\n",
    "art_control_agg=aggregate(df=art_control)\n",
    "art_cc_agg=aggregate(df=art_cc)\n",
    "\n",
    "display(art_control_agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Computing the Segmented Regression parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Here we define all the intervals between the mentioned event about the climate : the school strike of August 20, 2018 led by Greta Thunberg, the 2018 United Nations Climate Change Conference (COP24) on December 14, 2018 and the summit of the United Nations on September 23, 2019. The modularity of the code allows to simply add or put out an event without changing the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTERVALS =[[\"2018-01\",\"2018-07\"],[\"2018-08\",\"2018-11\"],[\"2018-12\",\"2019-08\"],[\"2019-09\",\"2020-02\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Here we define a function to compute multiple linear regression according to intervals given as arguments. The function returns a dict with the summary of the results, the predicted values corresponding to the given pageviews/month dataframe and the upper/lower values of the confidence intervals for each regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "import datetime\n",
    "from statsmodels.stats.outliers_influence import summary_table\n",
    "\n",
    "def list_LR_periods(df, intervals, CI=0.05) :\n",
    "    \"\"\"Compute the linear regression over all the periods given as a list parameters (intervals)\n",
    "    The confidence intervals (Mean) are extracted from the summary table. All the results are structured\n",
    "    in a dictionnary and given as output.\"\"\"\n",
    "    \n",
    "    results_dict={}\n",
    "    \n",
    "    #iterate through the intervals to compute their regression.\n",
    "    for interval, number_int in zip(intervals, list(range(1,len(intervals)+1,1))) :\n",
    "        #Linear regression with pageviews as dependent variable and months independent variables. \n",
    "        mod = smf.ols(formula=\"pageviews ~ months\", data=df.loc[interval[0]:interval[1]])\n",
    "        res=mod.fit()\n",
    "        #Compute the serie of predicted values from the basic linear equation with one variable : y = intercept + coef * x\n",
    "        pages_predict=res.params[0]+res.params[1]*df.loc[interval[0]:interval[1]][\"months\"]\n",
    "        #Extract the computed values (including upper and lower CI values)\n",
    "        _, summary_values, summary_names = summary_table(res, alpha=CI)\n",
    "        #Create a temporary dataframe with the result\n",
    "        df_res_tmp = pd.DataFrame(summary_values, columns=summary_names)\n",
    "        #display(df_res_tmp)  -> To see how the data is structured\n",
    "        #WARNING : It's a bit tricky but the column has always 95% in its name even if the CI\n",
    "        #introduce in alpha is different of 0.05. It has been tested and the values are different.\n",
    "        predict_ci_low = df_res_tmp[\"Mean ci\\n95% low\"].T\n",
    "        predict_ci_upp = df_res_tmp[\"Mean ci\\n95% upp\"].T\n",
    "        \n",
    "        result={\n",
    "            \"results\" : res,\n",
    "            \"predicts\" : pages_predict,\n",
    "            \"CI\" : {\n",
    "                \"lower\" : predict_ci_low,\n",
    "                \"upper\" : predict_ci_upp\n",
    "            }\n",
    "        }\n",
    "        results_dict[number_int]=result\n",
    "    return results_dict\n",
    "                                    \n",
    "\n",
    "        \n",
    "#       Create a dictionnary to pass the results. (See the structure below)\n",
    "\n",
    " \n",
    "    #Create a dictionnary to pass the results. The structure is as follow :\n",
    "#     dic = {\n",
    "#         \"interval1\" :{\n",
    "#             \"result\" : res_before,\n",
    "#             \"predicts\" : pages_predict_before,\n",
    "#             \"CI\" : {\n",
    "#                 \"lower\" : predict_ci_low_before,\n",
    "#                 \"upper\" : predict_ci_upp_before\n",
    "#             }\n",
    "#         }, \n",
    "#         \"interval2\" : {\n",
    "#             \"result\" : res_after,\n",
    "#             \"predicts\" : pages_predict_after,\n",
    "#             \"CI\" : {\n",
    "#                 \"lower\" : predict_ci_low_after,\n",
    "#                 \"upper\" : predict_ci_upp_after\n",
    "#             }\n",
    "#         },\n",
    "#         ...\n",
    "#     }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">We can try the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "art_control_reg=list_LR_periods(df=art_control_agg, intervals=INTERVALS)\n",
    "art_cc_reg=list_LR_periods(df=art_cc_agg, intervals=INTERVALS)\n",
    "\n",
    "print(f\"CC | Linear Regression interval 2 :\\n\\n {art_cc_reg[2]['results'].summary()}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Displaying the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Here we have a function to plot two dataframes, generally the studied group of Wikipedia articles and a selected comparator group. Each event is display with a vertical bar to separator each period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import (AutoMinorLocator, MultipleLocator)\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "def plot_comparator(dfs, lin_reg_res, params)  :\n",
    "    \"\"\"Function that plots the data to compare 2 ITS. It must be given the results of \n",
    "    the linear regression and the timeserie with the numbering by month. As the linear\n",
    "    regression function, it is possible to specify the interval of months for each period.\n",
    "    All the params are given through dictionnary. The dataframes are given as a tuple and the\n",
    "    corresponding results of the linear regression as well. The confidence area for each linear\n",
    "    regression is plotted as an area.\n",
    "    As follow : (df_studied, df_comparator)\"\"\"\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~ Extract parameter ~~~~~~~~~~~~~~~~~~~~~~\n",
    "    keys=params.keys()\n",
    "    title=params[\"title\"] if \"title\" in keys else \"\"\n",
    "    xlabel=params[\"xlabel\"] if \"xlabel\" in keys else \"\"\n",
    "    ylabel=params[\"ylabel\"] if \"ylabel\" in keys else \"\"\n",
    "    filename=params[\"filename\"] if \"filename\" in keys else \"no_name\"\n",
    "    intervals=params[\"intervals\"] if \"intervals\" in keys else []#raise(\"No intervals\")\n",
    "    #Names used to add in the legend\n",
    "    names=params[\"names\"] if \"names\" in keys else [\"\", \"\"]\n",
    "    ci=params[\"ci\"] if \"ci\" in keys else \"\"\n",
    "    #axis limits\n",
    "    TOP=params[\"top\"] if \"top\" in keys else None\n",
    "    BOTTOM=params[\"bottom\"] if \"bottom\" in keys else None\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~ Fix limit of the axes ~~~~~~~~~~~~~~~~~~~~~~ \n",
    "    \n",
    "    #Boundaries of the y-axis, it has been decided to keep them constant instead of relative by % of the\n",
    "    #max and min value because it is easier to compare plot when the axes are constant\n",
    "    BOTTOM=0 if BOTTOM is None else BOTTOM\n",
    "    all_pageviews=[j for i in [list(dfs[0][\"pageviews\"]), list(dfs[1][\"pageviews\"])] for j in i]\n",
    "    MAX=max(all_pageviews)\n",
    "    TOP=MAX+0.1*MAX if TOP is None else TOP#1500000#100000000 \n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~ Create the empty plot ~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    fig, ax = plt.subplots(1,1,figsize= (11,7), sharey = True, sharex = True)\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~ Plotting all the data points of the serie ~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    #Choose in advance the colors of the two groups\n",
    "#     colors=[(\"black\", \"dimgray\"), (\"darkgrey\", \"silver\")]\n",
    "    colors=[(\"darkred\", \"firebrick\", \"indianred\"), (\"mediumaquamarine\", \"aquamarine\", \"paleturquoise\")]\n",
    "    #Plots the two plots, the second plot is the comparator group\n",
    "    for df, reg, col, name in zip(dfs, lin_reg_res, colors, names) :\n",
    "        #Just to have the correct keys for the reg dict and select\n",
    "        #properly the right period\n",
    "        name_interval=[\"-\".join(inter) for inter in intervals]\n",
    "        \n",
    "        ax.scatter(x=df[\"months\"],y=df[\"pageviews\"], marker=\"o\", s=50, c=col[0], label=f\"{name}-related Articles\")\n",
    "        #Plotting linear regression\n",
    "        for interval, n_inter, key in zip(intervals, name_interval,  list(range(1,len(intervals)+1,1))) :\n",
    "            ax.plot(df.loc[interval[0]:interval[1]][\"months\"],\n",
    "                    reg[key][\"predicts\"], #Take the data points of the regression from the dict\n",
    "                    linewidth=4,\n",
    "                    alpha=0.9,\n",
    "                    c=col[1],\n",
    "                    label=f\"{name} Article Trend\") #{n_inter}\n",
    "        \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~ CI plot ~~~~~~~~~~~~~~~~~~~~~~\n",
    "        #For each regression line, plot the upper CI values as a line and\n",
    "        #the lower CI values. Filled the area between the two.\n",
    "        periods = reg.values()\n",
    "        for period, interval in zip(periods, intervals) :\n",
    "            for _, ci_array in period[\"CI\"].items() :\n",
    "                ax.plot(df.loc[interval[0]:interval[1]][\"months\"],\n",
    "                        ci_array,\n",
    "                        linewidth=2,\n",
    "                        alpha=0.1,\n",
    "                        c=col[2]\n",
    "                       )\n",
    "            #Create a color area representing the CI\n",
    "            plt.fill_between(df.loc[interval[0]:interval[1]][\"months\"],\n",
    "                period[\"CI\"][\"lower\"],\n",
    "                period[\"CI\"][\"upper\"],\n",
    "                where=period[\"CI\"][\"upper\"] >= period[\"CI\"][\"lower\"],\n",
    "                facecolor=col[2], alpha=0.25, interpolate=True, label=f\"{name} Confidence Interval ({ci})\")\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~ Visual modifications of the plot ~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    ax.set_ylim(bottom=BOTTOM, top=TOP)\n",
    "    ax.set_xlabel(xlabel, fontsize=12)\n",
    "    ax.set_ylabel(ylabel, fontsize=12)\n",
    "    ax.xaxis.grid(False)\n",
    "    ax.xaxis.set_major_locator(MultipleLocator(2))\n",
    "    ax.xaxis.set_minor_locator(AutoMinorLocator(4))\n",
    "    ax.xaxis.set_tick_params(length = 5, width = 1)\n",
    "    ax.yaxis.set_tick_params(length = 5, width = 1)\n",
    "    #Plot tick bars\n",
    "    ax.ticklabel_format(useOffset=False, style='plain')\n",
    "    #Plot a vertical line to delimit the 2 intervals.\n",
    "    #-0.5 to locate the mid of the month. (Since the frequency is end of month)\n",
    "    for interval, counter in zip(intervals, list(range(1,len(intervals)+1,1)))  :\n",
    "        #The event is located always as the first month of an interval except the first one\n",
    "        if counter > 1 :\n",
    "            event=float(df.loc[interval[0]][\"months\"]-0.5)\n",
    "            plt.axvline(x=event, color = 'black', alpha=1, linewidth=3.5)\n",
    "            #Indicate the mid June (Arbitrary addition values to center the label)\n",
    "            ax.text(event-1.1, TOP, interval[0], fontsize=13)\n",
    "    #Use to avoid \"duplicates\" in the legend\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    plt.legend(by_label.values(), by_label.keys(), loc='upper left',fontsize=12,bbox_to_anchor=(0.135, -0.15),ncol=2,frameon=True,edgecolor=\"black\")\n",
    "    fig.tight_layout()\n",
    "    fig.suptitle(title, y=1.07, fontsize=20)\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~ Save the plot in order to plot it further ~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    plt.savefig(f\"./plots/{filename}\", format=\"png\",bbox_inches='tight')\n",
    "    \n",
    "    \n",
    "    \n",
    "#Use the function \n",
    "params={\n",
    "    \"title\" : \"Climate articles vs. Comparator articles (Popular)\",\n",
    "    \"xlabel\" : \"Time (Months)\",\n",
    "    \"ylabel\" : \"Total Views (All Articles)\",\n",
    "    \"filename\" : \"climate_vs_comparator2.png\",\n",
    "    \"ci\" : \"95%\",\n",
    "    \"names\" : (\"Terrorism\",\"Security\"),\n",
    "    \"intervals\" : INTERVALS\n",
    "}\n",
    "dfs=(art_cc_agg, art_control_agg)\n",
    "regs=(art_cc_reg, art_control_reg)\n",
    "plot_comparator(dfs=dfs, lin_reg_res=regs, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ada_test_2] *",
   "language": "python",
   "name": "conda-env-ada_test_2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
